# Importing necessary libraries
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import pandas as pd
import nltk
nltk.download('punkt')

# Sample dataset (replace with your actual dataset)
data = [
    "speech text 1 here",
    "speech text 2 here",
    "speech text 3 here",
    # Add more speeches as needed
]

# Tokenize the data
tokenized_data = [word_tokenize(text.lower()) for text in data]

# Create Word2Vec model
model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)

# Example: Getting embedding vector for a word
word = "example"
vector = model.wv[word]

print(f"Embedding vector for '{word}':\n{vector}")

# Example: Finding similar words
similar_words = model.wv.most_similar(word)
print(f"Words similar to '{word}':\n{similar_words}")
