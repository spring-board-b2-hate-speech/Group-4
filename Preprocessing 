import re
import string
from nltk.tokenize import word_tokenize

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Remove usernames (assuming usernames start with @)
    text = re.sub(r'@[\w_]+', '', text)
    
    # Remove hashtags (assuming hashtags start with #)
    text = re.sub(r'#\w+', '', text)
    
    # Remove punctuation characters
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    
    # Tokenize the text into words
    tokens = word_tokenize(text)
    
    return tokens

# Example usage
input_text = "Hey @user1 check out this #awesome link: https://example.com! It's really cool!"
processed_tokens = preprocess_text(input_text)

print(processed_tokens)
