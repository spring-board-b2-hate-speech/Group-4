{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert_with_Lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importing essential libraries for data manipulation, machine learning, and NLP\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import numpy as np  # Numerical computing\n",
    "from tqdm import tqdm  # Progress bars for loops\n",
    "from transformers import BertTokenizer, BertModel  # BERT tokenizer and model for NLP\n",
    "import torch  # PyTorch for tensor computation and model handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the CSV file into a pandas DataFrame\n",
    "file_path = \"/content/ghc_train.csv\"  # Path to the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify successful loading\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fill any missing values in the 'text' column with an empty string\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# Define the features (X) and the target variable (y)\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 20% of the data is used for testing, and 80% for training\n",
    "# Stratify ensures that each split maintains the proportion of classes in the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Display the size of the training and testing sets\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  # For numerical operations and array manipulations\n",
    "import tensorflow as tf  # TensorFlow library for building and training neural networks\n",
    "from tensorflow.keras.models import Sequential  # Sequential model type for Keras\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout  # Layers used in the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Callback for early stopping during training\n",
    "from sklearn.utils.class_weight import compute_class_weight  # Function to compute class weights for imbalanced data\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # Metrics for evaluating model performance\n",
    "import seaborn as sns  # For creating statistical graphics\n",
    "import matplotlib.pyplot as plt  # For plotting data and evaluation results\n",
    "from transformers import BertTokenizer, TFBertModel  # BERT tokenizer and model from Hugging Face\n",
    "\n",
    "# Initializing the BERT tokenizer with the pre-trained 'bert-base-uncased' model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initializing the BERT model with the pre-trained 'bert-base-uncased' model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
    "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
    "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
    "You will be able to reuse this secret in all of your notebooks.\n",
    "Please note that authentication is recommended but still optional to access public models or datasets.\n",
    "  warnings.warn(\n",
    "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]\n",
    "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
    "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\n",
    "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "  warnings.warn(\n",
    "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]\n",
    "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]\n",
    "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
    "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "All the weights of TFBertModel were initialized from the PyTorch model.\n",
    "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to encode text data using a BERT tokenizer\n",
    "def encode_texts(texts, tokenizer, max_length=100):\n",
    "    \"\"\"\n",
    "    Encode a list of texts using the BERT tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    texts (pd.Series): Series of text data to encode.\n",
    "    tokenizer (BertTokenizer): Pre-trained BERT tokenizer.\n",
    "    max_length (int): Maximum length of tokenized sequences.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing input_ids and attention_mask tensors.\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),          # Convert Series to list of texts\n",
    "        truncation=True,         # Truncate sequences longer than max_length\n",
    "        padding='max_length',    # Pad sequences to max_length\n",
    "        max_length=max_length,   # Maximum length of the sequences\n",
    "        return_tensors='tf'      # Return TensorFlow tensors\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "# Encoding the training and testing text data with a maximum length of 50\n",
    "X_train_encodings = encode_texts(X_train, tokenizer, max_length=50)\n",
    "X_test_encodings = encode_texts(X_test, tokenizer, max_length=50)\n",
    "\n",
    "# Function to get BERT embeddings from encoded text data\n",
    "def get_bert_embeddings(encodings, bert_model):\n",
    "    \"\"\"\n",
    "    Obtain BERT embeddings for the encoded text data.\n",
    "\n",
    "    Parameters:\n",
    "    encodings (dict): Encoded text data containing input_ids and attention_mask.\n",
    "    bert_model (TFBertModel): Pre-trained BERT model.\n",
    "\n",
    "    Returns:\n",
    "    tf.Tensor: Tensor containing BERT embeddings.\n",
    "    \"\"\"\n",
    "    outputs = bert_model(\n",
    "        encodings['input_ids'],    # Input token IDs\n",
    "        attention_mask=encodings['attention_mask']  # Attention mask\n",
    "    )\n",
    "    return outputs.last_hidden_state  # Return the embeddings from the last hidden state\n",
    "\n",
    "# Setting the batch size for processing\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List to store BERT embeddings for the training data\n",
    "X_train_embeddings = []\n",
    "\n",
    "# Process the training data in batches to get BERT embeddings\n",
    "for i in range(0, len(X_train_encodings['input_ids']), batch_size):\n",
    "    # Create a batch of encodings\n",
    "    batch_encodings = {key: val[i:i+batch_size] for key, val in X_train_encodings.items()}\n",
    "    # Get BERT embeddings for the current batch\n",
    "    batch_embeddings = get_bert_embeddings(batch_encodings, bert_model)\n",
    "    # Append the batch embeddings to the list\n",
    "    X_train_embeddings.append(batch_embeddings)\n",
    "\n",
    "# Concatenate all the batch embeddings into a single tensor\n",
    "X_train_embeddings = tf.concat(X_train_embeddings, axis=0)\n",
    "\n",
    "# List to store BERT embeddings for the testing data\n",
    "X_test_embeddings = []\n",
    "\n",
    "# Process the testing data in batches to get BERT embeddings\n",
    "for i in range(0, len(X_test_encodings['input_ids']), batch_size):\n",
    "    # Create a batch of encodings\n",
    "    batch_encodings = {key: val[i:i+batch_size] for key, val in X_test_encodings.items()}\n",
    "    # Get BERT embeddings for the current batch\n",
    "    batch_embeddings = get_bert_embeddings(batch_encodings, bert_model)\n",
    "    # Append the batch embeddings to the list\n",
    "    X_test_embeddings.append(batch_embeddings)\n",
    "\n",
    "# Concatenate all the batch embeddings into a single tensor\n",
    "X_test_embeddings = tf.concat(X_test_embeddings, axis=0)\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# Convert class weights to a dictionary format required by TensorFlow\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Print class weights for verification\n",
    "print(\"Class weights:\", class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding a bidirectional LSTM layer with 256 units, returning sequences\n",
    "model.add(Bidirectional(LSTM(units=256, return_sequences=True, input_shape=(X_train_embeddings.shape[1], X_train_embeddings.shape[2]))))\n",
    "\n",
    "# Adding dropout to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Adding another bidirectional LSTM layer with 64 units, returning sequences\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "\n",
    "# Adding dropout to prevent overfitting\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Adding another bidirectional LSTM layer with 64 units, not returning sequences\n",
    "model.add(Bidirectional(LSTM(units=64)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a dense layer with 64 units and ReLU activation function\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding the final dense layer with 1 unit and sigmoid activation function for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_embeddings, y_train, epochs=5, batch_size=batch_size,\n",
    "                    validation_data=(X_test_embeddings, y_test),\n",
    "                    class_weight=class_weights_dict,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_embeddings, y_test, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test_embeddings)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=['Class 0', 'Class 1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
    "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
    "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
    "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
    "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
    "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
    "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
    "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
    "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "1102/1102 [==============================] - 443s 390ms/step - loss: 0.5381 - accuracy: 0.7320 - val_loss: 0.4959 - val_accuracy: 0.7947\n",
    "Epoch 2/5\n",
    "1102/1102 [==============================] - 424s 385ms/step - loss: 0.4686 - accuracy: 0.7822 - val_loss: 0.3700 - val_accuracy: 0.8142\n",
    "Epoch 3/5\n",
    "1102/1102 [==============================] - 426s 386ms/step - loss: 0.4434 - accuracy: 0.7869 - val_loss: 0.4812 - val_accuracy: 0.7661\n",
    "Epoch 4/5\n",
    "1102/1102 [==============================] - 426s 386ms/step - loss: 0.4171 - accuracy: 0.8071 - val_loss: 0.4239 - val_accuracy: 0.8169\n",
    "Epoch 5/5\n",
    "1102/1102 [==============================] - 427s 388ms/step - loss: 0.3905 - accuracy: 0.8200 - val_loss: 0.3814 - val_accuracy: 0.8373\n",
    "138/138 - 7s - loss: 0.3700 - accuracy: 0.8142 - 7s/epoch - 48ms/step\n",
    "\n",
    "Test accuracy: 0.8142014741897583\n",
    "138/138 [==============================] - 8s 51ms/step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " precision    recall  f1-score   support\n",
    "\n",
    "     Class 0       0.96      0.82      0.89      3874\n",
    "     Class 1       0.37      0.78      0.51       534\n",
    "\n",
    "    accuracy                           0.81      4408\n",
    "   macro avg       0.67      0.80      0.70      4408\n",
    "weighted avg       0.89      0.81      0.84      4408\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score,\n",
    "                             recall_score, precision_score, f1_score, roc_auc_score,\n",
    "                             roc_curve, auc)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "recall = recall_score(y_test, y_pred_classes)\n",
    "precision = precision_score(y_test, y_pred_classes)\n",
    "f1 = f1_score(y_test, y_pred_classes)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC AUC Score: {roc_auc}')\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc_value = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc_value:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.8142014519056261\n",
    "Recall: 0.7846441947565543\n",
    "Precision: 0.37310774710596617\n",
    "F1 Score: 0.5057332528666264\n",
    "ROC AUC Score: 0.8777224616622099\n",
    "Among all other dl models, the BERT with LSTM model is giving the best result with a good accuracy and a notable recall of 78%.The metric that iam considering is recall(i.e. Recall measures the proportion of true positive instances correctly identified by the model among all actual positive instances.) As there is an improvement in the metric that Iam considering that is Recall which is improved from 56% to 78% and The ROC AUC Score was also showing betterment than the finalized benchmark of the Machine Learning Model. Based on these, I Finalized BERT with LSTM model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
