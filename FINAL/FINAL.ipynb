{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I dont care about what it stands for or anythi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It's not a group it's an idea lol</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>So it's not just America!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The dog is a spectacular dancer considering he...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17591</th>\n",
              "      <td>I find rats nicer and cleaner than most Chinese</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17592</th>\n",
              "      <td>Check out this niggar, they hit things like wi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17593</th>\n",
              "      <td>this country has become an absolute shambles, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17594</th>\n",
              "      <td>Me aged 16 = anti-Semitism is bad Me aged 18 =...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17595</th>\n",
              "      <td>so messed up saying blacks don't deserve rights</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17596 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  Label\n",
              "0      Damn I thought they had strict gun laws in Ger...      0\n",
              "1      I dont care about what it stands for or anythi...      0\n",
              "2                      It's not a group it's an idea lol      0\n",
              "3                              So it's not just America!      0\n",
              "4      The dog is a spectacular dancer considering he...      0\n",
              "...                                                  ...    ...\n",
              "17591    I find rats nicer and cleaner than most Chinese      1\n",
              "17592  Check out this niggar, they hit things like wi...      1\n",
              "17593  this country has become an absolute shambles, ...      0\n",
              "17594  Me aged 16 = anti-Semitism is bad Me aged 18 =...      1\n",
              "17595    so messed up saying blacks don't deserve rights      0\n",
              "\n",
              "[17596 rows x 2 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data= pd.read_csv(\"dataset.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def data_cleaning(text):\n",
        "\n",
        "  text = re.sub(r'\\s+', ' ', text) #the re.sub function replaces one or more whitespace characters (\\s+) with a single space.\n",
        "\n",
        "  text = re.sub(r\"@\\S+\", \"\",text)\n",
        "\n",
        "  text = re.sub(r'#', '', text)\n",
        "\n",
        "  import contractions\n",
        "  text=contractions.fix(text)\n",
        "\n",
        "  text = text.lower()\n",
        "\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "  text = ' '.join([word for word in text.split() if len(word) > 2 or word.isnumeric()])\n",
        "\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "  return text\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "data['Text']=data['Text'].apply(data_cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split for train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X=data['Text']\n",
        "y=data['Label']\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encode label column with label encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "y_val = label_encoder.transform(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Tokenization using keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   8,   88,  207, ...,    0,    0,    0],\n",
              "       [ 211,  157,  831, ...,    0,    0,    0],\n",
              "       [ 464,    3,    4, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   1,    0,    0, ...,    0,    0,    0],\n",
              "       [ 174,    3, 8209, ...,    0,    0,    0],\n",
              "       [  10,  886, 1226, ...,    0,    0,    0]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=70,padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=70,padding='post')\n",
        "X_val_pad = pad_sequences(X_val_seq, maxlen=70,padding='post')\n",
        "\n",
        "X_train_pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = data['Text'].values\n",
        "labels = data['Label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "def one_hot_encoding(texts):\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "    embeddings = vectorizer.fit_transform(texts)\n",
        "    return embeddings\n",
        "\n",
        "embeddings_one_hot = one_hot_encoding(texts)\n",
        "X_train_one_hot, X_test_one_hot, y_train_one_hot, y_test_one_hot = train_test_split(embeddings_one_hot, labels, test_size=0.2, random_state=42)\n",
        "print(embeddings_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def term_frequency_encoding(texts):\n",
        "    vectorizer = CountVectorizer()\n",
        "    embeddings = vectorizer.fit_transform(texts)\n",
        "    return embeddings\n",
        "embeddings_tf = term_frequency_encoding(texts)\n",
        "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(embeddings_tf, labels, test_size=0.2, random_state=42)\n",
        "print(embeddings_tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tfidf_embedding(texts):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    embeddings = vectorizer.fit_transform(texts)\n",
        "    return embeddings\n",
        "\n",
        "embeddings_tfidf = tfidf_embedding(texts)\n",
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(embeddings_tfidf, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "print(embeddings_tfidf)\n",
        "print(embeddings_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BALANCING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(sampling_strategy='minority')\n",
        "X_train_pad, y_train = smote.fit_resample(X_train_pad,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"Embedded_data.csv\")\n",
        "X=data.drop(columns=['Label','Text','Tokens'])\n",
        "y=data['Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"Embedded_data.csv\")\n",
        "X=data.drop(columns=['Label','Text','Tokens'])\n",
        "y=data['Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
            "30 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [0.59690253        nan 0.6259591  0.6258878  0.62560345        nan\n",
            " 0.63981219 0.63988324 0.63547844        nan 0.64421719 0.6435778\n",
            " 0.64840909        nan 0.65110884 0.65025634 0.65132186        nan\n",
            " 0.65103769 0.65046951 0.66148116        nan 0.65416365 0.651535  ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Logistic Regression using Grid Search\n",
            "Precision: 0.639018691588785\n",
            "Recall: 0.38359046283309955\n",
            "Accuracy: 0.6625\n",
            "ROC-AUC Score: 0.6180129964595297\n",
            "Confusion Matrix:\n",
            " [[1785  309]\n",
            " [ 879  547]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score,accuracy_score,confusion_matrix,roc_auc_score\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Best Logistic Regression using Grid Search\")\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TUNNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best cross-validation accuracy: 0.6683454740549747\n",
            "Test set accuracy: 0.6673612426595946\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data= pd.read_csv(\"Embedded_data.csv\")\n",
        "\n",
        "\n",
        "X=data.drop(columns=['Label','Text','Tokens'])# Features\n",
        "y = data['Label']# Features\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression classifier\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the grid of hyperparameters to search\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Regularization penalty\n",
        "    'solver': ['liblinear', 'saga']  # Optimization algorithm\n",
        "}\n",
        "\n",
        "# Perform grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters and best score\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DL MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdXC5lMmG4Fg",
        "outputId": "10e69990-4fd5-4352-9573-e6d9459faa39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-62d0641268e2>:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_input_file,word2vec_output_file)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(400000, 200)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file='glove.6B.200d.txt'\n",
        "word2vec_output_file='word2vec.txt'\n",
        "glove2word2vec(glove_input_file,word2vec_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mx6VSBjeG8IU"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model= KeyedVectors.load_word2vec_format(word2vec_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PYoc5D-7HKAU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "texts = data['Text'].values\n",
        "labels = data['Label'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=15000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "X = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(X, maxlen=200)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rG3ci93UHLBD"
      },
      "outputs": [],
      "source": [
        "def glove_embeddings(filepath, word_index, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(filepath, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_dim = 200\n",
        "glove_filepath = 'glove.6B.200d.txt'\n",
        "embedding_matrix = glove_embeddings(glove_filepath, tokenizer.word_index, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ijReVTYHOSz",
        "outputId": "f06e3d9f-055f-4b50-a898-260da5cd36bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "220/220 [==============================] - 1997s 9s/step - loss: 0.6414 - accuracy: 0.6252\n",
            "Epoch 2/10\n",
            "220/220 [==============================] - 1991s 9s/step - loss: 0.5572 - accuracy: 0.7111\n",
            "Epoch 3/10\n",
            "220/220 [==============================] - 2005s 9s/step - loss: 0.4971 - accuracy: 0.7594\n",
            "Epoch 4/10\n",
            "220/220 [==============================] - 1998s 9s/step - loss: 0.4208 - accuracy: 0.8095\n",
            "Epoch 5/10\n",
            "220/220 [==============================] - 1997s 9s/step - loss: 0.3418 - accuracy: 0.8553\n",
            "Epoch 6/10\n",
            "220/220 [==============================] - 1995s 9s/step - loss: 0.2690 - accuracy: 0.8927\n",
            "Epoch 7/10\n",
            "220/220 [==============================] - 1999s 9s/step - loss: 0.2053 - accuracy: 0.9214\n",
            "Epoch 8/10\n",
            "220/220 [==============================] - 1944s 9s/step - loss: 0.1600 - accuracy: 0.9429\n",
            "Epoch 9/10\n",
            "220/220 [==============================] - 1951s 9s/step - loss: 0.1324 - accuracy: 0.9562\n",
            "Epoch 10/10\n",
            "220/220 [==============================] - 1946s 9s/step - loss: 0.1070 - accuracy: 0.9653\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ccc255f7c10>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Activation\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=200))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(256,activation='sigmoid'))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64,callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF5vMaj8msre",
        "outputId": "6ab63a62-c242-46b3-afcf-824fbfdf8d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "110/110 [==============================] - 177s 2s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.74      0.74      2094\n",
            "           1       0.62      0.62      0.62      1426\n",
            "\n",
            "    accuracy                           0.69      3520\n",
            "   macro avg       0.68      0.68      0.68      3520\n",
            "weighted avg       0.69      0.69      0.69      3520\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
        "y_pred = y_pred.flatten()\n",
        "print(classification_report(y_test,y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
