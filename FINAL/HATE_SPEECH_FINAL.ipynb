{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our hate speech detection model involves collecting and preprocessing the data. This step is crucial as the quality and cleanliness of the data significantly affect the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dont care about what it stands for or anythi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's not a group it's an idea lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So it's not just America!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>I find rats nicer and cleaner than most Chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>Check out this niggar, they hit things like wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shambles, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>Me aged 16 = anti-Semitism is bad Me aged 18 =...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying blacks don't deserve rights</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "0      Damn I thought they had strict gun laws in Ger...      0\n",
       "1      I dont care about what it stands for or anythi...      0\n",
       "2                      It's not a group it's an idea lol      0\n",
       "3                              So it's not just America!      0\n",
       "4      The dog is a spectacular dancer considering he...      0\n",
       "...                                                  ...    ...\n",
       "17591    I find rats nicer and cleaner than most Chinese      1\n",
       "17592  Check out this niggar, they hit things like wi...      1\n",
       "17593  this country has become an absolute shambles, ...      0\n",
       "17594  Me aged 16 = anti-Semitism is bad Me aged 18 =...      1\n",
       "17595    so messed up saying blacks don't deserve rights      0\n",
       "\n",
       "[17596 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data= pd.read_csv(\"dataset.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing the dataset, we can infer that we need to clean and transform the raw text data into a format suitable for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def data_cleaning(text):\n",
    "\n",
    "  text = re.sub(r'\\s+', ' ', text) #the re.sub function replaces one or more whitespace characters (\\s+) with a single space.\n",
    "\n",
    "  text = re.sub(r\"@\\S+\", \"\",text)\n",
    "\n",
    "  text = re.sub(r'#', '', text)\n",
    "\n",
    "  import contractions\n",
    "  text=contractions.fix(text)\n",
    "\n",
    "  text = text.lower()\n",
    "\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "  text = ' '.join([word for word in text.split() if len(word) > 2 or word.isnumeric()])\n",
    "\n",
    "  from nltk.stem import WordNetLemmatizer\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "  return text\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "data['Text']=data['Text'].apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12317,), (2640,), (2639,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data['Text']\n",
    "y=data['Label']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Target Label using LabelEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 1, 1, 0], dtype=int64),\n",
       " array([1, 1, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([1, 0, 1, ..., 1, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=70,padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=70,padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=70,padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding in the context of deep learning and natural language processing (NLP) is a way of representing words or phrases as dense vectors in a continuous vector space. These vectors capture semantic meanings and relationships between words. Embeddings transform the sparse, high-dimensional data of words into a lower-dimensional space, where similar words have similar vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ry981\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "data  = pd.read_csv(r\"D:\\dataset.csv\")\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['Tokens']=data['Text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Text'].values\n",
    "labels = data['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4634)\t1\n",
      "  (0, 18506)\t1\n",
      "  (0, 18453)\t1\n",
      "  (0, 8227)\t1\n",
      "  (0, 17690)\t1\n",
      "  (0, 8173)\t1\n",
      "  (0, 10461)\t1\n",
      "  (0, 9215)\t1\n",
      "  (0, 7736)\t1\n",
      "  (1, 5579)\t1\n",
      "  (1, 2972)\t1\n",
      "  (1, 394)\t1\n",
      "  (1, 20131)\t1\n",
      "  (1, 9788)\t1\n",
      "  (1, 17485)\t1\n",
      "  (1, 7220)\t1\n",
      "  (1, 13004)\t1\n",
      "  (1, 1096)\t1\n",
      "  (1, 9800)\t1\n",
      "  (1, 3977)\t1\n",
      "  (1, 18653)\t1\n",
      "  (1, 10719)\t1\n",
      "  (1, 18404)\t1\n",
      "  (1, 16541)\t1\n",
      "  (2, 9788)\t1\n",
      "  :\t:\n",
      "  (17594, 6498)\t1\n",
      "  (17594, 123)\t1\n",
      "  (17594, 53)\t1\n",
      "  (17594, 6298)\t1\n",
      "  (17594, 3914)\t1\n",
      "  (17594, 15001)\t1\n",
      "  (17594, 17530)\t1\n",
      "  (17594, 16061)\t1\n",
      "  (17594, 9902)\t1\n",
      "  (17594, 1058)\t1\n",
      "  (17594, 697)\t1\n",
      "  (17594, 68)\t1\n",
      "  (17594, 12285)\t1\n",
      "  (17594, 16320)\t1\n",
      "  (17594, 1074)\t1\n",
      "  (17594, 15570)\t1\n",
      "  (17594, 11487)\t1\n",
      "  (17595, 17055)\t1\n",
      "  (17595, 19474)\t1\n",
      "  (17595, 5562)\t1\n",
      "  (17595, 2188)\t1\n",
      "  (17595, 5030)\t1\n",
      "  (17595, 16061)\t1\n",
      "  (17595, 11517)\t1\n",
      "  (17595, 15632)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "def one_hot_encoding(texts):\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_one_hot = one_hot_encoding(texts)\n",
    "X_train_one_hot, X_test_one_hot, y_train_one_hot, y_test_one_hot = train_test_split(embeddings_one_hot, labels, test_size=0.2, random_state=42)\n",
    "print(embeddings_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4634)\t1\n",
      "  (0, 18506)\t1\n",
      "  (0, 18453)\t1\n",
      "  (0, 8227)\t1\n",
      "  (0, 17690)\t1\n",
      "  (0, 8173)\t1\n",
      "  (0, 10461)\t1\n",
      "  (0, 9215)\t1\n",
      "  (0, 7736)\t1\n",
      "  (1, 5579)\t1\n",
      "  (1, 2972)\t1\n",
      "  (1, 394)\t1\n",
      "  (1, 20131)\t1\n",
      "  (1, 9788)\t1\n",
      "  (1, 17485)\t1\n",
      "  (1, 7220)\t1\n",
      "  (1, 13004)\t1\n",
      "  (1, 1096)\t1\n",
      "  (1, 9800)\t1\n",
      "  (1, 3977)\t1\n",
      "  (1, 18653)\t1\n",
      "  (1, 10719)\t1\n",
      "  (1, 18404)\t1\n",
      "  (1, 16541)\t1\n",
      "  (2, 9788)\t2\n",
      "  :\t:\n",
      "  (17594, 6498)\t1\n",
      "  (17594, 123)\t1\n",
      "  (17594, 53)\t1\n",
      "  (17594, 6298)\t1\n",
      "  (17594, 3914)\t1\n",
      "  (17594, 15001)\t1\n",
      "  (17594, 17530)\t1\n",
      "  (17594, 16061)\t1\n",
      "  (17594, 9902)\t1\n",
      "  (17594, 1058)\t3\n",
      "  (17594, 697)\t3\n",
      "  (17594, 68)\t1\n",
      "  (17594, 12285)\t1\n",
      "  (17594, 16320)\t3\n",
      "  (17594, 1074)\t1\n",
      "  (17594, 15570)\t1\n",
      "  (17594, 11487)\t1\n",
      "  (17595, 17055)\t1\n",
      "  (17595, 19474)\t1\n",
      "  (17595, 5562)\t1\n",
      "  (17595, 2188)\t1\n",
      "  (17595, 5030)\t1\n",
      "  (17595, 16061)\t1\n",
      "  (17595, 11517)\t1\n",
      "  (17595, 15632)\t1\n"
     ]
    }
   ],
   "source": [
    "def term_frequency_encoding(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "embeddings_tf = term_frequency_encoding(texts)\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(embeddings_tf, labels, test_size=0.2, random_state=42)\n",
    "print(embeddings_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7736)\t0.3699627936790537\n",
      "  (0, 9215)\t0.1388506619303749\n",
      "  (0, 10461)\t0.3752596291015289\n",
      "  (0, 8173)\t0.41649038488028367\n",
      "  (0, 17690)\t0.47406059292584823\n",
      "  (0, 8227)\t0.2567293667498387\n",
      "  (0, 18453)\t0.15165197423800875\n",
      "  (0, 18506)\t0.3200116412881795\n",
      "  (0, 4634)\t0.3369195893330076\n",
      "  (1, 16541)\t0.4642690706501393\n",
      "  (1, 18404)\t0.09161042575119856\n",
      "  (1, 10719)\t0.16223888994565513\n",
      "  (1, 18653)\t0.0996453675495539\n",
      "  (1, 3977)\t0.4299948872975778\n",
      "  (1, 9800)\t0.23717831092225916\n",
      "  (1, 1096)\t0.25979668623580054\n",
      "  (1, 13004)\t0.18352720527959232\n",
      "  (1, 7220)\t0.144594064993865\n",
      "  (1, 17485)\t0.38989666328939804\n",
      "  (1, 9788)\t0.1259968016515918\n",
      "  (1, 20131)\t0.17923531261954773\n",
      "  (1, 394)\t0.18577715954742052\n",
      "  (1, 2972)\t0.26841797205609746\n",
      "  (1, 5579)\t0.2645688794841402\n",
      "  (2, 10867)\t0.44695575945206567\n",
      "  :\t:\n",
      "  (17594, 5562)\t0.06812392525584945\n",
      "  (17594, 5524)\t0.09892682229959995\n",
      "  (17594, 8418)\t0.05777122162500571\n",
      "  (17594, 13091)\t0.07406630708554074\n",
      "  (17594, 11373)\t0.20105832019565742\n",
      "  (17594, 12134)\t0.06097233025985478\n",
      "  (17594, 7901)\t0.08336507193940645\n",
      "  (17594, 14979)\t0.07590268520423042\n",
      "  (17594, 12820)\t0.04273516384786676\n",
      "  (17594, 20603)\t0.09812191669890655\n",
      "  (17594, 9047)\t0.06341183727703134\n",
      "  (17594, 9739)\t0.0865632426967268\n",
      "  (17594, 18653)\t0.037650979930335456\n",
      "  (17594, 20131)\t0.06772402294456893\n",
      "  (17594, 394)\t0.07019585834892103\n",
      "  (17594, 2972)\t0.10142167094521107\n",
      "  (17594, 9215)\t0.047587902310253255\n",
      "  (17595, 15632)\t0.3803264224031014\n",
      "  (17595, 11517)\t0.5397848851279821\n",
      "  (17595, 16061)\t0.35263459967341154\n",
      "  (17595, 5030)\t0.38260149254900994\n",
      "  (17595, 2188)\t0.35234175746755136\n",
      "  (17595, 5562)\t0.2397110948188069\n",
      "  (17595, 19474)\t0.2556041267640127\n",
      "  (17595, 17055)\t0.21519626987595328\n",
      "(17596, 20763)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_embedding(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(texts)\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(embeddings_tfidf, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(embeddings_tfidf)\n",
    "print(embeddings_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17965496  0.21911526  0.07153209 ... -0.03757478  0.22783396\n",
      "  -0.3270442 ]\n",
      " [ 0.2585517   0.29105872  0.11257217 ... -0.03062421  0.52885604\n",
      "  -0.5240098 ]\n",
      " [ 0.1331658   0.27762625  0.38334575 ...  0.06959226  0.69906306\n",
      "  -0.5330936 ]\n",
      " ...\n",
      " [ 0.0509892   0.32648292  0.23927818 ... -0.05263434  0.3443746\n",
      "  -0.29707268]\n",
      " [ 0.13551329  0.26237762  0.1619319  ... -0.02249849  0.491493\n",
      "  -0.4308026 ]\n",
      " [ 0.21669094  0.3248966   0.02764771 ... -0.03923914  0.545161\n",
      "  -0.43017298]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "def word2vec_embedding_cbow(texts):\n",
    "    model = Word2Vec(texts, vector_size=300, window=5, min_count=1, workers=4,sg=0)\n",
    "    word_vectors = model.wv\n",
    "    #print(word_vectors)\n",
    "\n",
    "    def get_word2vec_embeddings(text, word_vectors):\n",
    "        embeddings = [word_vectors[word] for word in text if word in word_vectors]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(300)\n",
    "\n",
    "    embeddings = np.array([get_word2vec_embeddings(text, word_vectors) for text in texts])\n",
    "    return embeddings\n",
    "\n",
    "embeddings_w2v_cbow = word2vec_embedding_cbow(data['Tokens'])\n",
    "print(embeddings_w2v_cbow)\n",
    "X_train_w2v1, X_test_w2v1, y_train_w2v1, y_test_w2v1 = train_test_split(embeddings_w2v_cbow, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00337505 -0.08936203  0.01494806 ... -0.06897631 -0.11581796\n",
      "  -0.16027392]\n",
      " [ 0.01793438 -0.10054299  0.05521449 ... -0.09753586 -0.17237748\n",
      "  -0.24999352]\n",
      " [ 0.08088573 -0.0762444  -0.02521128 ... -0.09672859 -0.05041833\n",
      "  -0.27526566]\n",
      " ...\n",
      " [ 0.06851795 -0.03317465 -0.01209099 ... -0.02324978  0.02682451\n",
      "  -0.21082322]\n",
      " [ 0.06000196 -0.09641755  0.06982504 ... -0.09364914 -0.1057099\n",
      "  -0.22627196]\n",
      " [ 0.09124887 -0.05448456  0.0516753  ... -0.10454585 -0.1026437\n",
      "  -0.2512715 ]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec_embedding_sg(texts):\n",
    "    model = Word2Vec(texts, vector_size=200, window=6, min_count=1, workers=4,sg=1)\n",
    "    word_vectors = model.wv\n",
    "    #print(word_vectors)\n",
    "\n",
    "    def get_word2vec_embeddings(text, word_vectors):\n",
    "        embeddings = [word_vectors[word] for word in text if word in word_vectors]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(200)\n",
    "\n",
    "    embeddings = np.array([get_word2vec_embeddings(text, word_vectors) for text in texts])\n",
    "    return embeddings\n",
    "\n",
    "embeddings_w2v_sg = word2vec_embedding_sg(data['Tokens'])\n",
    "print(embeddings_w2v_sg)\n",
    "X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2 = train_test_split(embeddings_w2v_sg, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"Embedded_data.csv\")\n",
    "X=data.drop(columns=['Label','Text','Tokens'])\n",
    "y=data['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "30 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [0.59690253        nan 0.6259591  0.6258878  0.62560345        nan\n",
      " 0.63981219 0.63988324 0.63547844        nan 0.64421719 0.6435778\n",
      " 0.64840909        nan 0.65110884 0.65025634 0.65132186        nan\n",
      " 0.65103769 0.65046951 0.66148116        nan 0.65416365 0.651535  ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression using Grid Search\n",
      "Precision: 0.639018691588785\n",
      "Recall: 0.38359046283309955\n",
      "Accuracy: 0.6625\n",
      "ROC-AUC Score: 0.6180129964595297\n",
      "Confusion Matrix:\n",
      " [[1785  309]\n",
      " [ 879  547]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score,accuracy_score,confusion_matrix,roc_auc_score\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Best Logistic Regression using Grid Search\")\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6488636363636363\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example dataset (you should replace this with your own dataset)\n",
    "# Assume you have a CSV file 'data.csv' with features and target columns\n",
    "# Replace 'Embedded_data.csv' with your dataset file path\n",
    "data = pd.read_csv('Embedded_data.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X=data.drop(columns=['Label','Text','Tokens'])# Features\n",
    "y = data['Label']                # Target variable\n",
    "\n",
    "# Encoding categorical data (if 'Label' is categorical)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Printing the classification report\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data= pd.read_csv(\"Embedded_data.csv\")\n",
    "\n",
    "\n",
    "X=data.drop(columns=['Label','Text','Tokens'])# Features\n",
    "y = data['Label']# Features\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # K=3, you can choose any value of K\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best cross-validation accuracy: 0.6683454740549747\n",
      "Test set accuracy: 0.6673612426595946\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data= pd.read_csv(\"Embedded_data.csv\")\n",
    "\n",
    "\n",
    "X=data.drop(columns=['Label','Text','Tokens'])# Features\n",
    "y = data['Label']# Features\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the Logistic Regression classifier\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "    'solver': ['liblinear', 'saga']  # Optimization algorithm\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and best score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'metric': 'manhattan', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.6341651902705496\n",
      "Test set accuracy: 0.6181094904337943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "data= pd.read_csv(\"Embedded_data.csv\")\n",
    "\n",
    "\n",
    "X=data.drop(columns=['Label','Text','Tokens'])# Features\n",
    "y = data['Label']# Features\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define grid of hyperparameters\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],  # K values to try\n",
    "    'weights': ['uniform', 'distance'],  # Weighting scheme for neighbors\n",
    "    'metric': ['euclidean', 'manhattan']  # Distance metric\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and best score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "540 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.65170135 0.65567925 0.66160671\n",
      " 0.65178212 0.65632857 0.66046972 0.65348729 0.65803393 0.66095638\n",
      " 0.65438022 0.65705948 0.66168673 0.65210663 0.66055079 0.66258038\n",
      " 0.65632873 0.66152426 0.66347343 0.66128069 0.65973838 0.66298553\n",
      " 0.65868371 0.66201259 0.66339249 0.65835798 0.65860142 0.66128161\n",
      " 0.65251267 0.65308204 0.65746519 0.64699303 0.6565723  0.65876475\n",
      " 0.64885945 0.65843971 0.66022609 0.65373133 0.65673523 0.66103735\n",
      " 0.6551923  0.65705948 0.66136235 0.65730266 0.65754722 0.66079391\n",
      " 0.65673533 0.66120021 0.66095664 0.65746503 0.65762806 0.66355463\n",
      " 0.65486769 0.66014442 0.66087544        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66241837 0.6620119  0.66420408 0.65876426 0.66201236 0.66420375\n",
      " 0.65892742 0.662093   0.66574659 0.66282362 0.6640427  0.66444726\n",
      " 0.66371697 0.66193037 0.66241821 0.65908943 0.66225521 0.66184966\n",
      " 0.66298665 0.66314919 0.66314816 0.65957641 0.66615299 0.66696396\n",
      " 0.66006368 0.66128135 0.66331113 0.65884566 0.66184959 0.66144375\n",
      " 0.66136238 0.66030729 0.66209238 0.66014459 0.66022566 0.66339276\n",
      " 0.6599021  0.66225518 0.66217411 0.6618495  0.65852111 0.66444749\n",
      " 0.66095624 0.66201197 0.65998248 0.66152485 0.65697828 0.66071241\n",
      " 0.65738514 0.65884552 0.65917003 0.65965757 0.6577087  0.65981939\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.6505639  0.66071336 0.65608539\n",
      " 0.65502977 0.65835854 0.66201183 0.65178219 0.66160704 0.66128158\n",
      " 0.6549487  0.6564101  0.66420382 0.65754738 0.66014528 0.66038839\n",
      " 0.65186405 0.66322956 0.66306739 0.65730226 0.66298639 0.66193103\n",
      " 0.65527321 0.6617682  0.66103771 0.65884562 0.66144408 0.66566575\n",
      " 0.65145834 0.65722307 0.65860152 0.65308168 0.66534068 0.6607945\n",
      " 0.65251248 0.65843988 0.66428581 0.65389363 0.65957697 0.66371677\n",
      " 0.65446287 0.6616053  0.66282435 0.65705928 0.66006349 0.66379745\n",
      " 0.65908943 0.65567898 0.66233648 0.66128168 0.66371727 0.66517824\n",
      " 0.659414   0.65998172 0.66306696        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65421864 0.65835795 0.65697943 0.65283722 0.65835815 0.65787219\n",
      " 0.6535677  0.65803443 0.66298579 0.65161952 0.65860248 0.66103748\n",
      " 0.65072647 0.65925216 0.65957644 0.65243177 0.65925143 0.66168762\n",
      " 0.66111858 0.66160599 0.66355421 0.65397447 0.65941387 0.66249901\n",
      " 0.6568166  0.66322923 0.66282365 0.65397375 0.65340672 0.65600521\n",
      " 0.6524316  0.65608565 0.6594956  0.655436   0.65567935 0.66022592\n",
      " 0.65502944 0.65787143 0.66030712 0.65535513 0.66014535 0.6609571\n",
      " 0.6591699  0.66006319 0.66347333 0.65454311 0.66298619 0.66103758\n",
      " 0.65592242 0.65827718 0.66168736 0.65998242 0.66217516 0.65998202]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best cross-validation accuracy: 0.666963956161581\n",
      "Test set accuracy: 0.6635726463345331\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Embedded_data.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X=data.drop(columns=['Label','Text','Tokens'])# Features\n",
    "y = data['Label']  \n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and best score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry981\\AppData\\Local\\Temp\\ipykernel_9716\\2477500379.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file,word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file=r\"C:\\Users\\ry981\\glove.6B.200d.txt\"\n",
    "word2vec_output_file=r\"D:\\word2vec.txt.txt\"\n",
    "glove2word2vec(glove_input_file,word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model= KeyedVectors.load_word2vec_format(word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PYoc5D-7HKAU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"D:\\dataset.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "texts = data['Text'].values\n",
    "labels = data['Label'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, maxlen=200)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding using GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "GloVe.6B.200d.txt is one of the pre-trained GloVe models, where:\n",
    "\n",
    "6B: The model was trained on a corpus of 6 billion tokens (words).\n",
    "\n",
    "200d: Each word is represented by a 200-dimensional vector.\n",
    "\n",
    "Details of GloVe.6B.200d.txt:\n",
    "\n",
    "Corpus: Common Crawl (a dataset containing 6 billion tokens).\n",
    "\n",
    "Vocabulary Size: 400,000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rG3ci93UHLBD"
   },
   "outputs": [],
   "source": [
    "def glove_embeddings(filepath, word_index, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 200\n",
    "glove_filepath =r\"C:\\Users\\ry981\\glove.6B.200d.txt\"\n",
    "embedding_matrix = glove_embeddings(glove_filepath, tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ijReVTYHOSz",
    "outputId": "f06e3d9f-055f-4b50-a898-260da5cd36bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 898ms/step - accuracy: 0.6106 - loss: 0.6545\n",
      "Epoch 2/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 965ms/step - accuracy: 0.7179 - loss: 0.5210\n",
      "Epoch 3/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 939ms/step - accuracy: 0.7947 - loss: 0.4189\n",
      "Epoch 4/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 966ms/step - accuracy: 0.8474 - loss: 0.3389\n",
      "Epoch 5/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 963ms/step - accuracy: 0.8984 - loss: 0.2458\n",
      "Epoch 6/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 980ms/step - accuracy: 0.9292 - loss: 0.1779\n",
      "Epoch 7/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 988ms/step - accuracy: 0.9431 - loss: 0.1372\n",
      "Epoch 8/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.9618 - loss: 0.1004\n",
      "Epoch 9/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 1s/step - accuracy: 0.9693 - loss: 0.0800\n",
      "Epoch 10/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 1s/step - accuracy: 0.9748 - loss: 0.0685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19bc217bf10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Activation\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=200))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kF5vMaj8msre",
    "outputId": "6ab63a62-c242-46b3-afcf-824fbfdf8d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74      2094\n",
      "           1       0.62      0.63      0.63      1426\n",
      "\n",
      "    accuracy                           0.70      3520\n",
      "   macro avg       0.69      0.69      0.69      3520\n",
      "weighted avg       0.70      0.70      0.70      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
    "y_pred = y_pred.flatten()\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
