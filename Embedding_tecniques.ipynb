{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization with NLTK\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure text column is of type string\n",
    "train_data['text'] = train_data['text'].astype(str)\n",
    "\n",
    "# Download required NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Apply word tokenization to each text entry in the DataFrame\n",
    "train_data['tokenized_text'] = train_data['text'].apply(word_tokenize)\n",
    "\n",
    "# Display the DataFrame with the new tokenized_text column\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Drop rows with missing values in the 'text' column\n",
    "train_data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Function to compute TF-IDF embeddings\n",
    "def compute_tfidf_embeddings(corpus):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_embeddings = tfidf_vectorizer.fit_transform(corpus)\n",
    "    return tfidf_embeddings\n",
    "\n",
    "# Extract text data and compute TF-IDF embeddings\n",
    "text_data = train_data['text'].values\n",
    "tfidf_embeddings = compute_tfidf_embeddings(text_data)\n",
    "\n",
    "# Print the TF-IDF embeddings\n",
    "print(tfidf_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: TF-IDF Encoding\n",
    "def compute_tfidf_embeddings(corpus):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_embeddings = tfidf_vectorizer.fit_transform(corpus)\n",
    "    return tfidf_embeddings\n",
    "\n",
    "# Extract text and target variable\n",
    "texts = train_data['text'].values\n",
    "target = train_data['hd'].values\n",
    "\n",
    "# Compute TF-IDF embeddings\n",
    "tfidf_embeddings = compute_tfidf_embeddings(texts)\n",
    "\n",
    "# Step 2: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_embeddings, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predictions and Evaluation\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Extract text data from the training DataFrame\n",
    "text_data = train_data['text'].values\n",
    "\n",
    "# Replace NaN values with empty strings\n",
    "text_data = np.where(pd.isnull(text_data), '', text_data)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_texts = [word_tokenize(text) for text in text_data]\n",
    "\n",
    "# Train a Word2Vec model on the tokenized texts\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to compute average Word2Vec embeddings for each document\n",
    "def average_word2vec(tokens, model, vocab, vector_dim):\n",
    "    vec_sum = np.zeros((vector_dim,), dtype=\"float32\")\n",
    "    num_tokens = 0\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            num_tokens += 1\n",
    "            vec_sum = np.add(vec_sum, model.wv[token])\n",
    "    if num_tokens > 0:\n",
    "        vec_sum = np.divide(vec_sum, num_tokens)\n",
    "    return vec_sum\n",
    "\n",
    "# Generate average embeddings for each text in the training set\n",
    "vocabulary_set = set(word2vec_model.wv.index_to_key)\n",
    "text_embeddings = np.array([average_word2vec(tokens, word2vec_model, vocabulary_set, 100) for tokens in tokenized_texts])\n",
    "\n",
    "# Print the resulting embeddings\n",
    "print(text_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Display the first two rows of the DataFrame and its info\n",
    "print(train_data.head(2))\n",
    "train_data.info()\n",
    "\n",
    "# Ensure 'label' column is treated as string type\n",
    "train_data['text'] = train_data['label'].astype(str)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "\n",
    "# Fit and transform the 'label' column to one-hot encoded format\n",
    "encoded_labels = encoder.fit_transform(train_data[['label']])\n",
    "\n",
    "# Print the one-hot encoded array\n",
    "print(encoded_labels)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
