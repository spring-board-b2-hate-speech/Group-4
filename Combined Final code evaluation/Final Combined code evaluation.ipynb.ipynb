{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298a83ed",
   "metadata": {},
   "source": [
    "PROJECT:- Hate speech Recognition in online group chat rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801840fc",
   "metadata": {},
   "source": [
    "# # Data Preprocessing\n",
    "The first step in our hate speech detection model involves collecting and preprocessing the data. This step is crucial as the quality and cleanliness of the data significantly affect the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a478a",
   "metadata": {},
   "source": [
    "We use the provided dataset (HateSpeechDetection.csv), which contains text data labeled as hate speech (1) or not hate speech (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "057979bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dont care about what it stands for or anythi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's not a group it's an idea lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So it's not just America!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If ppl dont wear masks you complain... If ppl ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We should send them All back  to africa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Checking to see if it's whataboutism from the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>As a european, I approve this.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Idk which of these groups to join, they're all...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sometimes the messenger is just as important a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This is the future we all want.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wow the monkey calls someone else a monkey lol</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>They are the gypsies of America.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I know that guy!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Label\n",
       "0   Damn I thought they had strict gun laws in Ger...      0\n",
       "1   I dont care about what it stands for or anythi...      0\n",
       "2                   It's not a group it's an idea lol      0\n",
       "3                           So it's not just America!      0\n",
       "4   The dog is a spectacular dancer considering he...      0\n",
       "5   If ppl dont wear masks you complain... If ppl ...      0\n",
       "6             We should send them All back  to africa      1\n",
       "7   Checking to see if it's whataboutism from the ...      0\n",
       "8                      As a european, I approve this.      0\n",
       "9   Idk which of these groups to join, they're all...      0\n",
       "10  Sometimes the messenger is just as important a...      0\n",
       "11                    This is the future we all want.      0\n",
       "12     Wow the monkey calls someone else a monkey lol      1\n",
       "13                   They are the gypsies of America.      1\n",
       "14                                   I know that guy!      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"HateSpeechDetection.csv\")\n",
    "data.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20cacaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17586</th>\n",
       "      <td>africans should be in a zoo with the other ani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17587</th>\n",
       "      <td>I wouldn't want my daughter to marry a Chink</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17588</th>\n",
       "      <td>not anymore. they are like the shitty utube no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17589</th>\n",
       "      <td>Yet still, better for a 750cc to seek a win (o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17590</th>\n",
       "      <td>people this is new thread due to the impending...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>I find rats nicer and cleaner than most Chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>Check out this niggar, they hit things like wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shambles, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>Me aged 16 = anti-Semitism is bad Me aged 18 =...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying blacks don't deserve rights</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "17586  africans should be in a zoo with the other ani...      1\n",
       "17587       I wouldn't want my daughter to marry a Chink      1\n",
       "17588  not anymore. they are like the shitty utube no...      1\n",
       "17589  Yet still, better for a 750cc to seek a win (o...      0\n",
       "17590  people this is new thread due to the impending...      0\n",
       "17591    I find rats nicer and cleaner than most Chinese      1\n",
       "17592  Check out this niggar, they hit things like wi...      1\n",
       "17593  this country has become an absolute shambles, ...      0\n",
       "17594  Me aged 16 = anti-Semitism is bad Me aged 18 =...      1\n",
       "17595    so messed up saying blacks don't deserve rights      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2639c66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17596 entries, 0 to 17595\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    17596 non-null  object\n",
      " 1   Label   17596 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 275.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2a486",
   "metadata": {},
   "source": [
    "After observing the dataset, we can infer that we need to clean and transform the raw text data into a format suitable for our  model. This involves several sub-steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d490f",
   "metadata": {},
   "source": [
    "Removing Extra Spaces: Normalize the spacing in the text to remove any extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c59a9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text) #the re.sub function replaces one or more whitespace characters (\\s+) with a single space.\n",
    "data['Text'] = data['Text'].apply(remove_extra_spaces)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16839dd9",
   "metadata": {},
   "source": [
    "Remove usernames: Same as for the URL, a username in a text won’t give any valuable information because it won’t be recognized as a word carrying meaning. We will then remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cc30431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(text):\n",
    "    return re.sub(r\"@\\S+\", \"\",text) \n",
    "#We used pattern “@\\S+” -> it suggests string group which starts with ‘@’ and followed by non-whitespace character(\\S), ‘+’ means repeatition of preceding character one or more times\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_username)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523437a",
   "metadata": {},
   "source": [
    "Remove Hashtags: Hashtags are hard to apprehend, but usually contain useful information about the context of a text and its content. The problem with hashtags is that the words are all after the other, without a space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3757241b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>I find rats nicer and cleaner than most Chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>Check out this niggar, they hit things like wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shambles, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>Me aged 16 = anti-Semitism is bad Me aged 18 =...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying blacks don't deserve rights</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "17591    I find rats nicer and cleaner than most Chinese      1\n",
       "17592  Check out this niggar, they hit things like wi...      1\n",
       "17593  this country has become an absolute shambles, ...      0\n",
       "17594  Me aged 16 = anti-Semitism is bad Me aged 18 =...      1\n",
       "17595    so messed up saying blacks don't deserve rights      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#', '', text)\n",
    "# replacing the character(\"#\") with \"\" but not removing the term.\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_hashtags)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdec827",
   "metadata": {},
   "source": [
    "Lowercasing: Convert all text to lowercase to ensure uniformity, as the model should treat \"Hate\" and \"hate\" as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81013400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lower(text):\n",
    "    return text.lower()\n",
    "data['Text'] = data['Text'].apply(text_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d445a83",
   "metadata": {},
   "source": [
    "Removing Punctuation: Strip out punctuation to focus on the words themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e4287b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>i find rats nicer and cleaner than most chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>check out this niggar they hit things like wil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shambles t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>me aged 16  antisemitism is bad me aged 18  an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying blacks dont deserve rights</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "17591    i find rats nicer and cleaner than most chinese      1\n",
       "17592  check out this niggar they hit things like wil...      1\n",
       "17593  this country has become an absolute shambles t...      0\n",
       "17594  me aged 16  antisemitism is bad me aged 18  an...      1\n",
       "17595     so messed up saying blacks dont deserve rights      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "    #\\w: Represents any alphanumeric character (equivalent to [a-zA-Z0-9_]).\n",
    "    #\\s: Denotes any whitespace character, such as space, tab, or newline.\n",
    "    # so it defines the other than a alphanumeric character followed by a single space, ('^' for negation) remove other characters\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_punctuation)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef16c84",
   "metadata": {},
   "source": [
    "Remove URLs: URLs do not give any information when we try to analyze text from words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "049c78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "# it identifies the words starting with http or https or www and ending with a non-white space Character(\\S) then remove it\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a805b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b0ef2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[damn, i, thought, they, had, strict, gun, law...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, dont, care, about, what, it, stands, for, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[its, not, a, group, its, an, idea, lol]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[so, its, not, just, america]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, dog, is, a, spectacular, dancer, conside...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>[i, find, rats, nicer, and, cleaner, than, mos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>[check, out, this, niggar, they, hit, things, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>[this, country, has, become, an, absolute, sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>[me, aged, 16, antisemitism, is, bad, me, aged...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>[so, messed, up, saying, blacks, dont, deserve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "0      [damn, i, thought, they, had, strict, gun, law...      0\n",
       "1      [i, dont, care, about, what, it, stands, for, ...      0\n",
       "2               [its, not, a, group, its, an, idea, lol]      0\n",
       "3                          [so, its, not, just, america]      0\n",
       "4      [the, dog, is, a, spectacular, dancer, conside...      0\n",
       "...                                                  ...    ...\n",
       "17591  [i, find, rats, nicer, and, cleaner, than, mos...      1\n",
       "17592  [check, out, this, niggar, they, hit, things, ...      1\n",
       "17593  [this, country, has, become, an, absolute, sha...      0\n",
       "17594  [me, aged, 16, antisemitism, is, bad, me, aged...      1\n",
       "17595  [so, messed, up, saying, blacks, dont, deserve...      0\n",
       "\n",
       "[17596 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "data['Text'] = data['Text'].apply(word_tokenize)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9607602",
   "metadata": {},
   "source": [
    "Data claening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6904d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Remove rows with missing values from the DataFrame\n",
    "cleaned_data = train_data.dropna()\n",
    "\n",
    "# Separate features and target variable\n",
    "features = cleaned_data['text']\n",
    "target = cleaned_data['label']\n",
    "\n",
    "# Print the shape of the features and target arrays\n",
    "print(\"Shape of features (X):\", features.shape)\n",
    "print(\"Shape of target (y):\", target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4102880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "training_features, testing_features, training_labels, testing_labels = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42, stratify=target\n",
    ")\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training features shape:\", training_features.shape)\n",
    "print(\"Testing features shape:\", testing_features.shape)\n",
    "print(\"Training labels shape:\", training_labels.shape)\n",
    "print(\"Testing labels shape:\", testing_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e29f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer with specified parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf_matrix = tfidf_vectorizer.fit_transform(training_features)\n",
    "\n",
    "# Transform the test data based on the fitted TF-IDF vectorizer\n",
    "X_test_tfidf_matrix = tfidf_vectorizer.transform(testing_features)\n",
    "\n",
    "# Print shapes of TF-IDF matrices for training and testing sets\n",
    "print(\"Shape of training TF-IDF matrix:\", X_train_tfidf_matrix.shape)\n",
    "print(\"Shape of testing TF-IDF matrix:\", X_test_tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac366da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE for handling class imbalance\n",
    "smote_resampler = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data to generate synthetic samples\n",
    "X_resampled_features, y_resampled_labels = smote_resampler.fit_resample(X_train_tfidf_matrix, training_labels)\n",
    "\n",
    "# Display the distribution of the resampled labels\n",
    "print(\"Resampled label distribution:\\n\", pd.Series(y_resampled_labels).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f3f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "# Train the model using the resampled training data\n",
    "logistic_regression_model.fit(X_resampled_features, y_resampled_labels)\n",
    "\n",
    "# Optional: Print a message confirming that the model has been trained\n",
    "print(\"Logistic Regression model has been trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the RandomForestClassifier\n",
    "random_forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Optional: Print a message indicating the Random Forest model has been initialized\n",
    "print(\"Random Forest Classifier has been initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacdccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier with a fixed random seed\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the Random Forest model using the resampled data\n",
    "rf_classifier.fit(X_resampled_features, y_resampled_labels)\n",
    "\n",
    "# Optional: Print a message confirming that the Random Forest model has been trained\n",
    "print(\"Random Forest Classifier has been trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the RandomForestClassifier with a fixed random seed for reproducibility\n",
    "forest_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Optional: Print a message confirming the classifier initialization\n",
    "print(\"RandomForestClassifier instance has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06991d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test set\n",
    "test_accuracy = rf_classifier.score(X_test_tfidf_matrix, testing_labels)\n",
    "\n",
    "# Print the accuracy of the Random Forest Classifier on the test data\n",
    "print(f\"Accuracy of the Random Forest Classifier on the test set: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab012e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Make predictions using the trained Random Forest model\n",
    "predicted_labels = rf_classifier.predict(X_test_tfidf_matrix)\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "conf_matrix = confusion_matrix(testing_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Generate and print the classification report\n",
    "class_report = classification_report(testing_labels, predicted_labels)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate predictions on the test set using the trained Random Forest model\n",
    "predicted_labels = rf_classifier.predict(X_test_tfidf_matrix)\n",
    "\n",
    "# Create a classification report for the predictions\n",
    "classification_summary = classification_report(testing_labels, predicted_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 150,200,300,350,400], \n",
    "    'max_features': [1,2,'sqrt', 'log2', None], \n",
    "    'max_depth': [4, 6, 10,15,20], \n",
    "    'max_leaf_nodes': [2, 4, 6,12,20]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ca48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the Random Forest Classifier and parameter grid\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, \n",
    "                           verbose=2, \n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Optionally, fit GridSearchCV to the training data\n",
    "grid_search.fit(X_resampled_features, y_resampled_labels)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best parameters found:\\n\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set using the best model from GridSearchCV\n",
    "predicted_labels_best = grid_search.predict(X_test_tfidf_matrix)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report_summary = classification_report(testing_labels, predicted_labels_best)\n",
    "print(\"Classification Report:\\n\", report_summary)\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "confusion_mat = confusion_matrix(testing_labels, predicted_labels_best)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c137a62",
   "metadata": {},
   "source": [
    "Data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab71ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "\n",
    "# Assume train_data is already loaded into a pandas DataFrame\n",
    "# train_data = pd.read_csv('path_to_your_csv.csv')\n",
    "\n",
    "# Preprocess text data: Remove NaNs, clean text, and split data\n",
    "train_data = train_data.dropna(subset=['text', 'label'])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804365e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load your DataFrame (df)\n",
    "# df = pd.read_csv('path_to_your_csv.csv')\n",
    "\n",
    "# Count the number of columns in the DataFrame\n",
    "column_count = df.shape[1]\n",
    "print(f\"Number of columns: {column_count}\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply preprocessing to the 'comment' column\n",
    "df['processed_comment'] = df['comment'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df['processed_comment']\n",
    "y = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vectorize the text data (convert text to numerical features)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Print the shape of the data before sampling\n",
    "print(f\"Shape of data before sampling: {X_tfidf.shape}, {y.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "\n",
    "# Load your DataFrame (df)\n",
    "# df = pd.read_csv('path_to_your_csv.csv')\n",
    "\n",
    "# Count the number of columns in the DataFrame\n",
    "column_count = df.shape[1]\n",
    "print(f\"Number of columns: {column_count}\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply preprocessing to the 'comment' column\n",
    "df['processed_comment'] = df['comment'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df['processed_comment']\n",
    "y = df['label']\n",
    "\n",
    "# Vectorize the text data (convert text to numerical features)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Print the shape of the data before sampling\n",
    "print(f\"Shape of data before sampling: {X_tfidf.shape}, {y.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a K-Nearest Neighbors classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model after random oversampling\n",
    "print(\"Evaluation after Random Oversampling:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aba0f9",
   "metadata": {},
   "source": [
    "Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the new dataset\n",
    "dataset = pd.read_csv('new_processed_dataset.csv')\n",
    "print(dataset.head())\n",
    "\n",
    "# Drop rows with NaN values in the 'tweet' column\n",
    "dataset = dataset.dropna(subset=['tweet'])\n",
    "\n",
    "# Define the input features and target variable\n",
    "features = dataset['tweet']\n",
    "target = dataset['class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to display evaluation metrics\n",
    "def display_metrics(actual, predicted):\n",
    "    print(\"Accuracy:\", accuracy_score(actual, predicted))\n",
    "    print(\"Precision:\", precision_score(actual, predicted, average='weighted'))\n",
    "    print(\"Recall:\", recall_score(actual, predicted, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(actual, predicted, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fde53",
   "metadata": {},
   "source": [
    "Logistic encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a pipeline for text classification with CountVectorizer and LogisticRegression\n",
    "text_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(binary=True, max_features=1000)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "text_pipeline.fit(features_train, target_train)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = text_pipeline.predict(features_test)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(\"Evaluation using Count Vectorization and Logistic Regression:\")\n",
    "print(\"Accuracy Score:\", accuracy_score(target_test, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(target_test, predictions))\n",
    "\n",
    "# Function to display additional metrics\n",
    "def show_metrics(actual, predicted):\n",
    "    print(\"Accuracy:\", accuracy_score(actual, predicted))\n",
    "    print(\"Precision:\", precision_score(actual, predicted, average='weighted'))\n",
    "    print(\"Recall:\", recall_score(actual, predicted, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(actual, predicted, average='weighted'))\n",
    "\n",
    "# Display additional metrics\n",
    "show_metrics(target_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72770bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rom sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Encoding Pipeline\n",
    "pipeline_tfidf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_features=1000)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train and evaluate the model\n",
    "pipeline_tfidf.fit(X_train, y_train)\n",
    "y_pred_tfidf = pipeline_tfidf.predict(X_test)\n",
    "print(\"TF-IDF Encoding\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tfidf))\n",
    "print_metrics(y_test, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        tokenized_X = [tweet.split() for tweet in X]\n",
    "        self.model = Word2Vec(sentences=tokenized_X, vector_size=self.vector_size, window=self.window, min_count=self.min_count)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        def get_word2vec_features(text):\n",
    "            words = text.split()\n",
    "            feature_vector = np.mean([self.model.wv[word] for word in words if word in self.model.wv] or [np.zeros(self.vector_size)], axis=0)\n",
    "            return feature_vector\n",
    "        \n",
    "        return np.array([get_word2vec_features(tweet) for tweet in X])\n",
    "\n",
    "# Word2Vec Encoding Pipeline\n",
    "pipeline_w2v = Pipeline([\n",
    "    ('word2vec', Word2VecTransformer(vector_size=100)),  # We don't set max_features for Word2Vec\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train and evaluate the model\n",
    "pipeline_w2v.fit(X_train, y_train)\n",
    "y_pred_w2v = pipeline_w2v.predict(X_test)\n",
    "print(\"Word2Vec Encoding\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_w2v))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_w2v))\n",
    "print_metrics(y_test, y_pred_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1ff2d",
   "metadata": {},
   "source": [
    "Naivebays encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess data\n",
    "dataset = pd.read_csv('new_processed_dataset.csv')\n",
    "print(dataset.head())\n",
    "\n",
    "# Remove duplicates and handle missing values\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "features = dataset['tweet']\n",
    "target = dataset['class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to print evaluation metrics\n",
    "def display_metrics(actual, predicted):\n",
    "    print(\"Accuracy:\", accuracy_score(actual, predicted))\n",
    "    print(\"Precision:\", precision_score(actual, predicted, average='weighted'))\n",
    "    print(\"Recall:\", recall_score(actual, predicted, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(actual, predicted, average='weighted'))\n",
    "    # Instantiate a Multinomial Naive Bayes classifier pipeline with CountVectorizer\n",
    "nb_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=5000)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "nb_pipeline.fit(features_train, target_train)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = nb_pipeline.predict(features_test)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Evaluation Results:\")\n",
    "display_metrics(target_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a pipeline for text classification with CountVectorizer and LogisticRegression\n",
    "text_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(binary=True, max_features=1000)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "text_pipeline.fit(features_train, target_train)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = text_pipeline.predict(features_test)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(\"Evaluation using Count Vectorization and Logistic Regression:\")\n",
    "print(\"Accuracy Score:\", accuracy_score(target_test, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(target_test, predictions))\n",
    "\n",
    "# Function to display additional metrics\n",
    "def show_metrics(actual, predicted):\n",
    "    print(\"Accuracy:\", accuracy_score(actual, predicted))\n",
    "    print(\"Precision:\", precision_score(actual, predicted, average='weighted'))\n",
    "    print(\"Recall:\", recall_score(actual, predicted, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(actual, predicted, average='weighted'))\n",
    "\n",
    "# Display additional metrics\n",
    "show_metrics(target_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Create a pipeline for text classification with CountVectorizer and Multinomial Naive Bayes\n",
    "text_pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer(binary=True, max_features=1000)),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline using the training dataset\n",
    "text_pipeline.fit(features_train, target_train)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "test_predictions = text_pipeline.predict(features_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Text Classification Evaluation with Naive Bayes:\")\n",
    "print(\"Classification Report:\\n\", classification_report(target_test, test_predictions))\n",
    "\n",
    "# Define a function to print detailed performance metrics\n",
    "def print_performance_metrics(true_labels, predicted_labels):\n",
    "    print(\"Accuracy Score:\", accuracy_score(true_labels, predicted_labels))\n",
    "    print(\"Precision Score:\", precision_score(true_labels, predicted_labels, average='weighted'))\n",
    "    print(\"Recall Score:\", recall_score(true_labels, predicted_labels, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(true_labels, predicted_labels, average='weighted'))\n",
    "\n",
    "# Output the detailed performance metrics\n",
    "print_performance_metrics(target_test, test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548eba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a pipeline for text classification using TF-IDF and Multinomial Naive Bayes\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('tfidf_vectorizer', TfidfVectorizer(max_features=1000)),\n",
    "    ('naive_bayes_classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "tfidf_pipeline.fit(features_train, target_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "predicted_labels_tfidf = tfidf_pipeline.predict(features_test)\n",
    "\n",
    "# Print the classification report for TF-IDF encoding with Naive Bayes\n",
    "print(\"Evaluation of TF-IDF Encoding with Naive Bayes Classifier:\")\n",
    "print(\"Classification Report:\\n\", classification_report(target_test, predicted_labels_tfidf))\n",
    "\n",
    "# Function to display performance metrics\n",
    "def show_performance_metrics(true_values, predicted_values):\n",
    "    print(\"Accuracy:\", accuracy_score(true_values, predicted_values))\n",
    "    print(\"Precision:\", precision_score(true_values, predicted_values, average='weighted'))\n",
    "    print(\"Recall:\", recall_score(true_values, predicted_values, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(true_values, predicted_values, average='weighted'))\n",
    "\n",
    "# Display additional metrics for the TF-IDF pipeline results\n",
    "show_performance_metrics(target_test, predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d92916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Custom Transformer for Word2Vec\n",
    "class CustomWord2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.word2vec_model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        tokenized_texts = [text.split() for text in X]\n",
    "        self.word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=self.vector_size, window=self.window, min_count=self.min_count)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        def compute_feature_vector(text):\n",
    "            tokens = text.split()\n",
    "            vectors = [self.word2vec_model.wv[token] for token in tokens if token in self.word2vec_model.wv]\n",
    "            if not vectors:\n",
    "                return np.zeros(self.vector_size)\n",
    "            return np.mean(vectors, axis=0)\n",
    "        \n",
    "        return np.array([compute_feature_vector(text) for text in X])\n",
    "\n",
    "# Define the pipeline with the Word2Vec transformer and Gaussian Naive Bayes\n",
    "w2v_pipeline = Pipeline([\n",
    "    ('word2vec_transformer', CustomWord2VecTransformer(vector_size=100)),  # Reduced vector size for quicker processing\n",
    "    ('naive_bayes_classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "w2v_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "predicted_labels_w2v = w2v_pipeline.predict(X_test)\n",
    "\n",
    "# Print the classification report for the Word2Vec model\n",
    "print(\"Word2Vec Encoding with Naive Bayes Classifier:\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predicted_labels_w2v))\n",
    "\n",
    "# Function to display performance metrics\n",
    "def display_metrics(true_labels, predicted_labels):\n",
    "    print(\"Accuracy:\", accuracy_score(true_labels, predicted_labels))\n",
    "    print(\"Precision:\", precision_score(true_labels, predicted_labels, average='weighted'))\n",
    "    print(\"Recall:\", recall_score(true_labels, predicted_labels, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(true_labels, predicted_labels, average='weighted'))\n",
    "\n",
    "# Display detailed performance metrics\n",
    "display_metrics(y_test, predicted_labels_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b865ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the pipeline for Term Frequency encoding with Naive Bayes\n",
    "tf_pipeline = Pipeline([\n",
    "    ('tf_vectorizer', CountVectorizer(max_features=1000)),  # Convert text data into term frequency features\n",
    "    ('naive_bayes', MultinomialNB())  # Classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "tf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "predicted_labels_tf = tf_pipeline.predict(X_test)\n",
    "\n",
    "# Print the classification report for the Term Frequency encoding model\n",
    "print(\"Term Frequency Encoding with Naive Bayes Classifier:\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predicted_labels_tf))\n",
    "\n",
    "# Function to display additional performance metrics\n",
    "def display_performance_metrics(true_labels, predicted_labels):\n",
    "    print(\"Accuracy:\", accuracy_score(true_labels, predicted_labels))\n",
    "    print(\"Precision:\", precision_score(true_labels, predicted_labels, average='weighted'))\n",
    "    print(\"Recall:\", recall_score(true_labels, predicted_labels, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(true_labels, predicted_labels, average='weighted'))\n",
    "\n",
    "# Show detailed performance metrics\n",
    "display_performance_metrics(y_test, predicted_labels_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c913a3b",
   "metadata": {},
   "source": [
    "Embedding Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e246022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization with NLTK\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure text column is of type string\n",
    "train_data['text'] = train_data['text'].astype(str)\n",
    "\n",
    "# Download required NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Apply word tokenization to each text entry in the DataFrame\n",
    "train_data['tokenized_text'] = train_data['text'].apply(word_tokenize)\n",
    "\n",
    "# Display the DataFrame with the new tokenized_text column\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ccc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Drop rows with missing values in the 'text' column\n",
    "train_data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Function to compute TF-IDF embeddings\n",
    "def compute_tfidf_embeddings(corpus):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_embeddings = tfidf_vectorizer.fit_transform(corpus)\n",
    "    return tfidf_embeddings\n",
    "\n",
    "# Extract text data and compute TF-IDF embeddings\n",
    "text_data = train_data['text'].values\n",
    "tfidf_embeddings = compute_tfidf_embeddings(text_data)\n",
    "\n",
    "# Print the TF-IDF embeddings\n",
    "print(tfidf_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: TF-IDF Encoding\n",
    "def compute_tfidf_embeddings(corpus):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_embeddings = tfidf_vectorizer.fit_transform(corpus)\n",
    "    return tfidf_embeddings\n",
    "\n",
    "# Extract text and target variable\n",
    "texts = train_data['text'].values\n",
    "target = train_data['hd'].values\n",
    "\n",
    "# Compute TF-IDF embeddings\n",
    "tfidf_embeddings = compute_tfidf_embeddings(texts)\n",
    "\n",
    "# Step 2: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_embeddings, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predictions and Evaluation\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed631fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Extract text data from the training DataFrame\n",
    "text_data = train_data['text'].values\n",
    "\n",
    "# Replace NaN values with empty strings\n",
    "text_data = np.where(pd.isnull(text_data), '', text_data)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_texts = [word_tokenize(text) for text in text_data]\n",
    "\n",
    "# Train a Word2Vec model on the tokenized texts\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to compute average Word2Vec embeddings for each document\n",
    "def average_word2vec(tokens, model, vocab, vector_dim):\n",
    "    vec_sum = np.zeros((vector_dim,), dtype=\"float32\")\n",
    "    num_tokens = 0\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            num_tokens += 1\n",
    "            vec_sum = np.add(vec_sum, model.wv[token])\n",
    "    if num_tokens > 0:\n",
    "        vec_sum = np.divide(vec_sum, num_tokens)\n",
    "    return vec_sum\n",
    "\n",
    "# Generate average embeddings for each text in the training set\n",
    "vocabulary_set = set(word2vec_model.wv.index_to_key)\n",
    "text_embeddings = np.array([average_word2vec(tokens, word2vec_model, vocabulary_set, 100) for tokens in tokenized_texts])\n",
    "\n",
    "# Print the resulting embeddings\n",
    "print(text_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85242f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Display the first two rows of the DataFrame and its info\n",
    "print(train_data.head(2))\n",
    "train_data.info()\n",
    "\n",
    "# Ensure 'label' column is treated as string type\n",
    "train_data['text'] = train_data['label'].astype(str)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "\n",
    "# Fit and transform the 'label' column to one-hot encoded format\n",
    "encoded_labels = encoder.fit_transform(train_data[['label']])\n",
    "\n",
    "# Print the one-hot encoded array\n",
    "print(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4be4c",
   "metadata": {},
   "source": [
    "Model Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "\n",
    "# Load your DataFrame (df)\n",
    "# df = pd.read_csv('path_to_your_csv.csv')\n",
    "\n",
    "# Count the number of columns in the DataFrame\n",
    "column_count = df.shape[1]\n",
    "print(f\"Number of columns: {column_count}\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply preprocessing to the 'comment' column\n",
    "df['processed_comment'] = df['comment'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df['processed_comment']\n",
    "y = df['label']\n",
    "\n",
    "# Vectorize the text data (convert text to numerical features)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Print the shape of the data before sampling\n",
    "print(f\"Shape of data before sampling: {X_tfidf.shape}, {y.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = decision_tree_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation after Random Oversampling:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b882db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "\n",
    "# Load your DataFrame (data)\n",
    "# data = pd.read_csv('path_to_your_csv.csv')\n",
    "\n",
    "# Count the number of columns in the DataFrame\n",
    "column_count = data.shape[1]\n",
    "print(f\"Number of columns: {column_count}\")\n",
    "\n",
    "# Assuming the last column is the label and the rest are TF-IDF features\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data.iloc[:, -1]   # The last column\n",
    "\n",
    "# Print the shape of the data before sampling\n",
    "print(f\"Shape of data before sampling: {X.shape}, {y.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation after Random Oversampling:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Assuming 'data' is your DataFrame and it contains TF-IDF features and the label as the last column\n",
    "\n",
    "# Count the number of columns in the DataFrame\n",
    "column_count = data.shape[1]\n",
    "print(f\"Number of columns: {column_count}\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data.iloc[:, -1]   # The last column\n",
    "\n",
    "# Print the shape of the data before sampling\n",
    "print(f\"Shape of data before sampling: {X.shape}, {y.shape}\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation after Random Oversampling:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff68737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Assuming 'data' is your DataFrame and it contains TF-IDF features and the label as the last column\n",
    "\n",
    "# Count the number of columns in the DataFrame\n",
    "column_count = data.shape[1]\n",
    "print(f\"Number of columns: {column_count}\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data.iloc[:, -1]   # The last column\n",
    "\n",
    "# Print the shape of the data before sampling\n",
    "print(f\"Shape of data before sampling: {X.shape}, {y.shape}\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Instantiate and train a Decision Tree classifier\n",
    "decision_tree =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = decision_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcd6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Assuming 'data' is your DataFrame and it contains TF-IDF features and the label as the last column\n",
    "\n",
    "# Count the number of columns in the DataFrame\n",
    "column_count = data.shape[1]\n",
    "print(f\"Number of columns: {column_count}\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data.iloc[:, -1]   # The last column\n",
    "\n",
    "# Print the shape of the data before sampling\n",
    "print(f\"Shape of data before sampling: {X.shape}, {y.shape}\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Instantiate and train a Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Evaluation after Random Oversampling:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21cc6e",
   "metadata": {},
   "source": [
    "Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad93d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('new_processed_dataset.csv')\n",
    "print(data.head())\n",
    "\n",
    "# Handle any potential issues with data formats (e.g., strings that need to be converted)\n",
    "# This assumes that some columns might need conversion from string representations of lists or dictionaries\n",
    "data['features'] = data['features'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data['tweet']  # Assuming 'tweet' contains the text data\n",
    "y = data['class']  # Assuming 'class' contains the labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline that includes TF-IDF vectorization and Random Forest classification\n",
    "text_clf_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_features=1000),  # Convert text data to TF-IDF features\n",
    "    RandomForestClassifier(random_state=42)  # Classifier\n",
    ")\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832255b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the specified CSV file\n",
    "df = pd.read_csv('/content/cleaned_dataset_combined (2).csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify successful loading\n",
    "print(\"Displaying the first few rows of the DataFrame:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representation of list to actual list\n",
    "df['tweet_tokens'] = df['tweet_tokens'].apply(ast.literal_eval)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"\\nDataFrame after converting tweet_tokens to lists:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea066991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/cleaned_dataset_combined (2).csv')\n",
    "\n",
    "# Define feature and target variables\n",
    "X = df[['tweet_tokens']]\n",
    "y = df['class']\n",
    "\n",
    "# Convert list of tokens into a single string for each row\n",
    "# Assuming 'tweet_tokens' is a column where each entry is a list of tokens\n",
    "X['tweet_tokens'] = X['tweet_tokens'].apply(lambda tokens: ' '.join(eval(tokens)) if isinstance(tokens, str) else ' '.join(tokens))\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Transformed DataFrame:\")\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/cleaned_dataset_combined (2).csv')\n",
    "\n",
    "# Define feature and target variables\n",
    "X = df[['tweet_tokens']]\n",
    "y = df['class']\n",
    "\n",
    "# Convert list of tokens into a single string for each row\n",
    "# If 'tweet_tokens' is a string representation of a list, use eval to convert it\n",
    "X['tweet_tokens'] = X['tweet_tokens'].apply(lambda tokens: ' '.join(eval(tokens)) if isinstance(tokens, str) else ' '.join(tokens))\n",
    "\n",
    "# Create a ColumnTransformer to apply TfidfVectorizer to the 'tweet_tokens' column\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', TfidfVectorizer(), 'tweet_tokens')  # Apply TF-IDF vectorization\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns unchanged (though there are none in this case)\n",
    ")\n",
    "\n",
    "# Transform the feature data\n",
    "X_transformed = column_transformer.fit_transform(X)\n",
    "\n",
    "# Output the shape of the transformed data to verify\n",
    "print(\"Shape of the transformed feature data:\")\n",
    "print(X_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/cleaned_dataset_combined (2).csv')\n",
    "\n",
    "# Define feature and target variables\n",
    "X = df[['tweet_tokens']]\n",
    "y = df['class']\n",
    "\n",
    "# Convert list of tokens into a single string for each row\n",
    "X['tweet_tokens'] = X['tweet_tokens'].apply(lambda tokens: ' '.join(eval(tokens)) if isinstance(tokens, str) else ' '.join(tokens))\n",
    "\n",
    "# Create a ColumnTransformer to apply TfidfVectorizer to the 'tweet_tokens' column\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', TfidfVectorizer(), 'tweet_tokens')  # Apply TF-IDF vectorization\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns unchanged (though there are none in this case)\n",
    ")\n",
    "\n",
    "# Create a Random Forest classifier pipeline\n",
    "pipeline = make_pipeline(column_transformer, RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the first few rows of the training data and target\n",
    "print(\"\\nTraining Data:\")\n",
    "print(X_train.head())\n",
    "print(\"\\nTraining Target:\")\n",
    "print(y_train.head())\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Display classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/cleaned_dataset_combined (2).csv')\n",
    "\n",
    "# Define feature and target variables\n",
    "X = df[['tweet_tokens']]\n",
    "y = df['class']\n",
    "\n",
    "# Convert list of tokens into a single string for each row\n",
    "X['tweet_tokens'] = X['tweet_tokens'].apply(lambda tokens: ' '.join(eval(tokens)) if isinstance(tokens, str) else ' '.join(tokens))\n",
    "\n",
    "# Create a ColumnTransformer to apply TfidfVectorizer to the 'tweet_tokens' column\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', TfidfVectorizer(), 'tweet_tokens')  # Apply TF-IDF vectorization\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns unchanged (though there are none in this case)\n",
    ")\n",
    "\n",
    "# Create a Random Forest classifier pipeline\n",
    "pipeline = make_pipeline(column_transformer, RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f'\\nAccuracy: {accuracy:.4f}')\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
