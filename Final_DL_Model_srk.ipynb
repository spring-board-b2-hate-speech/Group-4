{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dont care about what it stands for or anythi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's not a group it's an idea lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So it's not just America!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>I find rats nicer and cleaner than most Chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>Check out this niggar, they hit things like wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shambles, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>Me aged 16 = anti-Semitism is bad Me aged 18 =...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying blacks don't deserve rights</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "0      Damn I thought they had strict gun laws in Ger...      0\n",
       "1      I dont care about what it stands for or anythi...      0\n",
       "2                      It's not a group it's an idea lol      0\n",
       "3                              So it's not just America!      0\n",
       "4      The dog is a spectacular dancer considering he...      0\n",
       "...                                                  ...    ...\n",
       "17591    I find rats nicer and cleaner than most Chinese      1\n",
       "17592  Check out this niggar, they hit things like wi...      1\n",
       "17593  this country has become an absolute shambles, ...      0\n",
       "17594  Me aged 16 = anti-Semitism is bad Me aged 18 =...      1\n",
       "17595    so messed up saying blacks don't deserve rights      0\n",
       "\n",
       "[17596 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"HateSpeechDetection.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\balus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\balus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\balus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def data_cleaning(text):\n",
    "  #Removing Extra Spaces:\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  \n",
    "  #Remove usernames:\n",
    "  text = re.sub(r\"@\\S+\", \"\",text)\n",
    "\n",
    "  #Remove Hashtags:\n",
    "  text = re.sub(r'#', '', text)\n",
    "\n",
    "  #Handling Contractions:\n",
    "  text=contractions.fix(text)\n",
    "\n",
    "  #Lowercasing:\n",
    "  text = text.lower()\n",
    "\n",
    "  #Removing Punctuation:\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "  #Remove URLs:\n",
    "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "  #Removing Short words\n",
    "  text = ' '.join([word for word in text.split() if len(word) > 2 or word.isnumeric()])\n",
    "\n",
    "  #Lemmatization\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "  return text\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "data['Text']=data['Text'].apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test - Validation Data Splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12317,), (3520,), (1759,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texts = data['Text'].values\n",
    "labels = data['Label'].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66666, random_state=42)\n",
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Target Label using LabelEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 1, 1, 0], dtype=int64),\n",
       " array([0, 0, 1, ..., 0, 0, 0], dtype=int64),\n",
       " array([1, 0, 1, ..., 0, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and Padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=200)\n",
    "X_val = pad_sequences(X_val, maxlen=200)\n",
    "X_test = pad_sequences(X_test, maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding using GloVe Embeddings:\n",
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. It was developed by researchers at Stanford University and is designed to capture the semantic relationships between words based on their co-occurrence in large text corpora.\n",
    "\n",
    "Key Features of GloVe:\n",
    "1. Co-occurrence Matrix: GloVe constructs a large matrix where each element represents how often a word pair co-occurs in the text corpus.\n",
    "2. Word Vectors: The co-occurrence matrix is factorized to produce a lower-dimensional representation of words, typically using methods like matrix factorization or stochastic gradient descent.\n",
    "3. Semantic Relationships: The resulting word vectors capture various linguistic regularities and patterns, such as analogies (e.g., king - man + woman ≈ queen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove.6B.200d.txt:\n",
    "GloVe.6B.200d.txt is one of the pre-trained GloVe models, where:\n",
    "\n",
    "6B: The model was trained on a corpus of 6 billion tokens (words).\n",
    "\n",
    "200d: Each word is represented by a 200-dimensional vector.\n",
    "\n",
    "Details of GloVe.6B.200d.txt:\n",
    "\n",
    "Corpus: Common Crawl (a dataset containing 6 billion tokens).\n",
    "\n",
    "Vocabulary Size: 400,000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def glove_embeddings(filepath, word_index, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 200\n",
    "glove_filepath = 'glove.6B.200d.txt' \n",
    "embedding_matrix = glove_embeddings(glove_filepath, tokenizer.word_index, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional-LSTM model:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6196s\u001b[0m 32s/step - loss: 0.6421 - roc_auc: 0.6208 - val_loss: 0.5701 - val_roc_auc: 0.7637\n",
      "Epoch 2/15\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 805ms/step - loss: 0.5061 - roc_auc: 0.8178 - val_loss: 0.5438 - val_roc_auc: 0.7872\n",
      "Epoch 3/15\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 842ms/step - loss: 0.4031 - roc_auc: 0.8921 - val_loss: 0.5851 - val_roc_auc: 0.7946\n",
      "Epoch 4/15\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 897ms/step - loss: 0.3026 - roc_auc: 0.9415 - val_loss: 0.6384 - val_roc_auc: 0.7693\n",
      "Epoch 5/15\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 865ms/step - loss: 0.1974 - roc_auc: 0.9754 - val_loss: 0.8184 - val_roc_auc: 0.7598\n",
      "Epoch 6/15\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 886ms/step - loss: 0.1384 - roc_auc: 0.9875 - val_loss: 1.0634 - val_roc_auc: 0.7558\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Embedding, Dropout, Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=200))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[AUC(name='roc_auc')])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_roc_auc', patience=3, restore_best_weights=True)\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=64,callbacks=[early_stopping], validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 190ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.75      0.76      2093\n",
      "           1       0.79      0.77      0.80      1427\n",
      "\n",
      "    accuracy                           0.85      3520\n",
      "   macro avg       0.84      0.76      0.75      3520\n",
      "weighted avg       0.86      0.78      0.77      3520\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
    "y_pred = y_pred.flatten()\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC-AUC Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC SCORE: 0.8265365915885402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc=roc_auc_score(y_test,y_pred)\n",
    "print('ROC-AUC SCORE:',roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.pth\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.pth\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'best_model.pth'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 200), dtype=tf.float32, name='keras_tensor_46')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2683409972768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341660128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341660304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341660480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341661184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341661360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341661536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341661712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341661888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341662064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341662768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341662944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341663120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341663296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341663472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341663648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341663824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341664000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2683341664176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "model.export('best_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
