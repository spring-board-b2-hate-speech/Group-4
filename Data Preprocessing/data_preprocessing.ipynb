{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801840fc",
   "metadata": {},
   "source": [
    "# # Data Preprocessing\n",
    "The first step in our hate speech detection model involves collecting and preprocessing the data. This step is crucial as the quality and cleanliness of the data significantly affect the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a478a",
   "metadata": {},
   "source": [
    "We use the provided dataset (HateSpeechDetection.csv), which contains text data labeled as hate speech (1) or not hate speech (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057979bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dont care about what it stands for or anythi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's not a group it's an idea lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So it's not just America!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If ppl dont wear masks you complain... If ppl ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We should send them All back  to africa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Checking to see if it's whataboutism from the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>As a european, I approve this.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Idk which of these groups to join, they're all...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sometimes the messenger is just as important a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This is the future we all want.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wow the monkey calls someone else a monkey lol</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>They are the gypsies of America.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I know that guy!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Label\n",
       "0   Damn I thought they had strict gun laws in Ger...      0\n",
       "1   I dont care about what it stands for or anythi...      0\n",
       "2                   It's not a group it's an idea lol      0\n",
       "3                           So it's not just America!      0\n",
       "4   The dog is a spectacular dancer considering he...      0\n",
       "5   If ppl dont wear masks you complain... If ppl ...      0\n",
       "6             We should send them All back  to africa      1\n",
       "7   Checking to see if it's whataboutism from the ...      0\n",
       "8                      As a european, I approve this.      0\n",
       "9   Idk which of these groups to join, they're all...      0\n",
       "10  Sometimes the messenger is just as important a...      0\n",
       "11                    This is the future we all want.      0\n",
       "12     Wow the monkey calls someone else a monkey lol      1\n",
       "13                   They are the gypsies of America.      1\n",
       "14                                   I know that guy!      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"HateSpeechDetection.csv\")\n",
    "data.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20cacaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17586</th>\n",
       "      <td>africans should be in a zoo with the other ani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17587</th>\n",
       "      <td>I wouldn't want my daughter to marry a Chink</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17588</th>\n",
       "      <td>not anymore. they are like the shitty utube no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17589</th>\n",
       "      <td>Yet still, better for a 750cc to seek a win (o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17590</th>\n",
       "      <td>people this is new thread due to the impending...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>I find rats nicer and cleaner than most Chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>Check out this niggar, they hit things like wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shambles, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>Me aged 16 = anti-Semitism is bad Me aged 18 =...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying blacks don't deserve rights</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "17586  africans should be in a zoo with the other ani...      1\n",
       "17587       I wouldn't want my daughter to marry a Chink      1\n",
       "17588  not anymore. they are like the shitty utube no...      1\n",
       "17589  Yet still, better for a 750cc to seek a win (o...      0\n",
       "17590  people this is new thread due to the impending...      0\n",
       "17591    I find rats nicer and cleaner than most Chinese      1\n",
       "17592  Check out this niggar, they hit things like wi...      1\n",
       "17593  this country has become an absolute shambles, ...      0\n",
       "17594  Me aged 16 = anti-Semitism is bad Me aged 18 =...      1\n",
       "17595    so messed up saying blacks don't deserve rights      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2639c66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17596 entries, 0 to 17595\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    17596 non-null  object\n",
      " 1   Label   17596 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 275.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2a486",
   "metadata": {},
   "source": [
    "After observing the dataset, we can infer that we need to clean and transform the raw text data into a format suitable for our  model. This involves several sub-steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d490f",
   "metadata": {},
   "source": [
    "Removing Extra Spaces: Normalize the spacing in the text to remove any extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c59a9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "data['Text'] = data['Text'].apply(remove_extra_spaces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16839dd9",
   "metadata": {},
   "source": [
    "Remove usernames: Same as for the URL, a username in a text won’t give any valuable information because it won’t be recognized as a word carrying meaning. We will then remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc30431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(text):\n",
    "    return re.sub(r'@[^ ]+', '', text)\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_username)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523437a",
   "metadata": {},
   "source": [
    "Remove Hashtags: Hashtags are hard to apprehend, but usually contain useful information about the context of a text and its content. The problem with hashtags is that the words are all after the other, without a space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3757241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#', '', text)\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c100739",
   "metadata": {},
   "source": [
    "Handling Contractions\n",
    "\n",
    "Handling contractions in text is an important step in text preprocessing, especially for tasks like hate speech detection where understanding the full meaning of the words is crucial. Contractions are shortened forms of words or combinations of words created by omitting certain letters and sounds (e.g., \"don't\" for \"do not\", \"I'm\" for \"I am\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17e449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "data['Text']=data['Text'].apply(lambda x:contractions.fix(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdec827",
   "metadata": {},
   "source": [
    "Lowercasing: Convert all text to lowercase to ensure uniformity, as the model should treat \"Hate\" and \"hate\" as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81013400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lower(text):\n",
    "    return text.lower()\n",
    "data['Text'] = data['Text'].apply(text_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d445a83",
   "metadata": {},
   "source": [
    "Removing Punctuation: Strip out punctuation to focus on the words themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e4287b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef16c84",
   "metadata": {},
   "source": [
    "Remove URLs: URLs do not give any information when we try to analyze text from words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049c78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a805b418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damn i thought they had strict gun laws in ger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i do not care about what it stands for or anyt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is not a group it is an idea lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so it is not just america</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>i find rats nicer and cleaner than most chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>check out this niggar they hit things like wil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shambles t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>me aged 16  antisemitism is bad me aged 18  an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying blacks do not deserve rights</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "0      damn i thought they had strict gun laws in ger...      0\n",
       "1      i do not care about what it stands for or anyt...      0\n",
       "2                    it is not a group it is an idea lol      0\n",
       "3                              so it is not just america      0\n",
       "4      the dog is a spectacular dancer considering he...      0\n",
       "...                                                  ...    ...\n",
       "17591    i find rats nicer and cleaner than most chinese      1\n",
       "17592  check out this niggar they hit things like wil...      1\n",
       "17593  this country has become an absolute shambles t...      0\n",
       "17594  me aged 16  antisemitism is bad me aged 18  an...      1\n",
       "17595   so messed up saying blacks do not deserve rights      0\n",
       "\n",
       "[17596 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ec680",
   "metadata": {},
   "source": [
    "Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form (lemma). It considers the context and transforms words into their meaningful base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d8a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\balui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\balui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\balui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f545c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizers(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    word=[]\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english'):\n",
    "            word.append(lemmatizer.lemmatize(i))\n",
    "        else:\n",
    "            word.append(i)\n",
    "    return ' '.join(word)\n",
    "data['Text'] = data['Text'].apply(lemmatizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81f93cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damn i thought they had strict gun law in germany</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i do not care about what it stand for or anyth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is not a group it is an idea lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so it is not just america</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>i find rat nicer and cleaner than most chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>check out this niggar they hit thing like wild...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shamble th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>me aged 16 antisemitism is bad me aged 18 anti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying black do not deserve right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "0      damn i thought they had strict gun law in germany      0\n",
       "1      i do not care about what it stand for or anyth...      0\n",
       "2                    it is not a group it is an idea lol      0\n",
       "3                              so it is not just america      0\n",
       "4      the dog is a spectacular dancer considering he...      0\n",
       "...                                                  ...    ...\n",
       "17591     i find rat nicer and cleaner than most chinese      1\n",
       "17592  check out this niggar they hit thing like wild...      1\n",
       "17593  this country has become an absolute shamble th...      0\n",
       "17594  me aged 16 antisemitism is bad me aged 18 anti...      1\n",
       "17595     so messed up saying black do not deserve right      0\n",
       "\n",
       "[17596 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9312711",
   "metadata": {},
   "source": [
    "Text Vectorization:\n",
    "Vectorization is the process of converting text into numerical representations. The TextVectorization layer is designed to standardize the text data, tokenize it, and convert it into integer sequences that can be used as input for deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f4e2dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        damn i thought they had strict gun law in germany\n",
       "1        i do not care about what it stand for or anyth...\n",
       "2                      it is not a group it is an idea lol\n",
       "3                                so it is not just america\n",
       "4        the dog is a spectacular dancer considering he...\n",
       "                               ...                        \n",
       "17591       i find rat nicer and cleaner than most chinese\n",
       "17592    check out this niggar they hit thing like wild...\n",
       "17593    this country has become an absolute shamble th...\n",
       "17594    me aged 16 antisemitism is bad me aged 18 anti...\n",
       "17595       so messed up saying black do not deserve right\n",
       "Name: Text, Length: 17596, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['Text']\n",
    "y = data[data.columns[1]].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a30256c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731df1d",
   "metadata": {},
   "source": [
    "Vocabulary Size (max_tokens=10000):\n",
    "\n",
    "By setting max_tokens to 10,000, we limit the vocabulary to the 10,000 most frequent words in the dataset. This helps in reducing the computational complexity and memory usage while retaining the most important words for the task.\n",
    "\n",
    "Sequence Length (output_sequence_length=350):\n",
    "\n",
    "The output_sequence_length parameter ensures that all text sequences are of equal length 35.0 tokens in this case). Shorter sequences will be padded (usually with zeros), and longer sequences will be truncated. This uniformity is necessary for efficient batch processing and model training.\n",
    "\n",
    "Integer Token Indices (output_mode='int'):\n",
    "\n",
    "The output_mode='int' setting indicates that the output will be integer indices of tokens. This is a common approach in NLP tasks, where each unique token in the vocabulary is assigned a unique integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25fe46f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(17596, 300), dtype=int64, numpy=\n",
       "array([[ 442,    7,  278, ...,    0,    0,    0],\n",
       "       [   7,   17,   10, ...,    0,    0,    0],\n",
       "       [  13,    4,   10, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  20,   75,   82, ...,    0,    0,    0],\n",
       "       [  39, 2203, 2795, ...,    0,    0,    0],\n",
       "       [  28, 2515,   60, ...,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer = TextVectorization(max_tokens=10000,\n",
    "                               output_sequence_length=300,\n",
    "                               output_mode='int')\n",
    "vectorizer.adapt(X.values)\n",
    "vectorized_text = vectorizer(X.values)\n",
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efca9b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'to',\n",
       " 'is',\n",
       " 'a',\n",
       " 'and',\n",
       " 'i',\n",
       " 'of',\n",
       " 'are',\n",
       " 'not',\n",
       " 'you',\n",
       " 'that',\n",
       " 'it',\n",
       " 'in',\n",
       " 'they',\n",
       " 'for',\n",
       " 'do',\n",
       " 'have',\n",
       " 'be',\n",
       " 'this',\n",
       " 'people',\n",
       " 'with',\n",
       " 'all',\n",
       " 'as',\n",
       " 'my',\n",
       " 'but',\n",
       " 'like',\n",
       " 'so',\n",
       " 'can',\n",
       " 'on',\n",
       " 'woman',\n",
       " 'just',\n",
       " 'if',\n",
       " 'their',\n",
       " 'we',\n",
       " 'them',\n",
       " 'was',\n",
       " 'will',\n",
       " 'me',\n",
       " 'am',\n",
       " 'black',\n",
       " 'what',\n",
       " 'would',\n",
       " 'or',\n",
       " 'who',\n",
       " 'about',\n",
       " 'there',\n",
       " 'at',\n",
       " 'from',\n",
       " 'no',\n",
       " 'your',\n",
       " 'get',\n",
       " 'because',\n",
       " 'fucking',\n",
       " 'he',\n",
       " 'should',\n",
       " 'when',\n",
       " 'one',\n",
       " 'she',\n",
       " 'up',\n",
       " 'think',\n",
       " 'more',\n",
       " 'want',\n",
       " 'by',\n",
       " 'how',\n",
       " 'out',\n",
       " 'an',\n",
       " 'white',\n",
       " 'why',\n",
       " 'being',\n",
       " 'know',\n",
       " 'our',\n",
       " 'these',\n",
       " 'muslim',\n",
       " 'country',\n",
       " 'some',\n",
       " 'men',\n",
       " 'u',\n",
       " 'even',\n",
       " 'make',\n",
       " 'her',\n",
       " 'has',\n",
       " 'fuck',\n",
       " 'only',\n",
       " 'hate',\n",
       " 'say',\n",
       " 'than',\n",
       " 'were',\n",
       " 'need',\n",
       " 'really',\n",
       " 'time',\n",
       " 'now',\n",
       " 'gay',\n",
       " 'here',\n",
       " 'go',\n",
       " 'see',\n",
       " 'any',\n",
       " 'most',\n",
       " 'good',\n",
       " 'those',\n",
       " 'shit',\n",
       " 'other',\n",
       " 'then',\n",
       " 'never',\n",
       " 'thing',\n",
       " 'does',\n",
       " 'right',\n",
       " 'look',\n",
       " 'way',\n",
       " 'many',\n",
       " 'did',\n",
       " 'been',\n",
       " 'jew',\n",
       " 'man',\n",
       " 'life',\n",
       " 'going',\n",
       " 'had',\n",
       " 'much',\n",
       " 'world',\n",
       " 'too',\n",
       " 'its',\n",
       " 'his',\n",
       " 'could',\n",
       " 'love',\n",
       " 'into',\n",
       " 'year',\n",
       " 'off',\n",
       " 'which',\n",
       " 'take',\n",
       " 'over',\n",
       " 'back',\n",
       " 'always',\n",
       " 'day',\n",
       " 'also',\n",
       " 'him',\n",
       " 'got',\n",
       " 'refugee',\n",
       " 'let',\n",
       " 'guy',\n",
       " 'very',\n",
       " 'come',\n",
       " 'such',\n",
       " 'well',\n",
       " 'actually',\n",
       " 'still',\n",
       " 'child',\n",
       " 'asian',\n",
       " 'immigrant',\n",
       " 'work',\n",
       " 'lot',\n",
       " 'race',\n",
       " 'kid',\n",
       " 'every',\n",
       " 'same',\n",
       " 'feel',\n",
       " 'better',\n",
       " 'believe',\n",
       " 'something',\n",
       " 'trans',\n",
       " 'where',\n",
       " 'girl',\n",
       " 'own',\n",
       " 'down',\n",
       " 'society',\n",
       " 'nothing',\n",
       " 'friend',\n",
       " 'mean',\n",
       " 'stop',\n",
       " 'person',\n",
       " 'bad',\n",
       " 'racist',\n",
       " 'someone',\n",
       " 'said',\n",
       " 'live',\n",
       " 'problem',\n",
       " 'anyone',\n",
       " 'tell',\n",
       " 'after',\n",
       " 'everyone',\n",
       " 'place',\n",
       " 'against',\n",
       " 'keep',\n",
       " 'care',\n",
       " 'real',\n",
       " 'bitch',\n",
       " 'anything',\n",
       " 'around',\n",
       " 'call',\n",
       " 'ever',\n",
       " 'wrong',\n",
       " 'new',\n",
       " 'first',\n",
       " 'give',\n",
       " 'saying',\n",
       " 'money',\n",
       " 'use',\n",
       " 'word',\n",
       " 'group',\n",
       " 'before',\n",
       " 'reason',\n",
       " 'nigger',\n",
       " 'family',\n",
       " 'getting',\n",
       " 'human',\n",
       " 'american',\n",
       " 'must',\n",
       " 'show',\n",
       " 'themselves',\n",
       " 'doing',\n",
       " 'school',\n",
       " 'great',\n",
       " 'enough',\n",
       " 'sure',\n",
       " 'end',\n",
       " 'indian',\n",
       " 'fact',\n",
       " 'long',\n",
       " 'job',\n",
       " 'find',\n",
       " 'having',\n",
       " 'though',\n",
       " 'while',\n",
       " 'help',\n",
       " 'everything',\n",
       " 'matter',\n",
       " 'best',\n",
       " 'kind',\n",
       " 'disgusting',\n",
       " 'made',\n",
       " 'used',\n",
       " 'trump',\n",
       " 'stupid',\n",
       " 'called',\n",
       " 'sex',\n",
       " 'try',\n",
       " 'kill',\n",
       " 'chinese',\n",
       " 'lol',\n",
       " 'again',\n",
       " 'next',\n",
       " 'home',\n",
       " 'point',\n",
       " 'government',\n",
       " 'big',\n",
       " 'another',\n",
       " 'god',\n",
       " 'old',\n",
       " 'issue',\n",
       " 'culture',\n",
       " 'jewish',\n",
       " 'put',\n",
       " 'trying',\n",
       " 'start',\n",
       " 'different',\n",
       " 'without',\n",
       " 'true',\n",
       " 'state',\n",
       " 'little',\n",
       " 'full',\n",
       " 'face',\n",
       " 'understand',\n",
       " 'le',\n",
       " 'die',\n",
       " 'away',\n",
       " 'literally',\n",
       " 'hope',\n",
       " 'few',\n",
       " 'allowed',\n",
       " 'last',\n",
       " 'told',\n",
       " 'medium',\n",
       " 'today',\n",
       " 'uk',\n",
       " 'faggot',\n",
       " 'talk',\n",
       " 'idea',\n",
       " 'else',\n",
       " 'thought',\n",
       " 'yes',\n",
       " 'living',\n",
       " 'house',\n",
       " 'female',\n",
       " 'until',\n",
       " 'seen',\n",
       " 'least',\n",
       " 'population',\n",
       " 'oh',\n",
       " 'liberal',\n",
       " 'two',\n",
       " 'hard',\n",
       " 'gender',\n",
       " 'free',\n",
       " 'dirty',\n",
       " 'change',\n",
       " 'part',\n",
       " 'others',\n",
       " 'hell',\n",
       " 'stay',\n",
       " 'lesbian',\n",
       " 'ugly',\n",
       " 'through',\n",
       " 'please',\n",
       " 'pretty',\n",
       " 'nice',\n",
       " 'deserve',\n",
       " 'worse',\n",
       " 'support',\n",
       " 'since',\n",
       " 'sick',\n",
       " 'war',\n",
       " 'saw',\n",
       " 'law',\n",
       " 'boy',\n",
       " 'might',\n",
       " 'yet',\n",
       " 'community',\n",
       " 'animal',\n",
       " 'wish',\n",
       " 'opinion',\n",
       " 'done',\n",
       " 'british',\n",
       " 'yourself',\n",
       " 'yeah',\n",
       " 'post',\n",
       " 'however',\n",
       " 'between',\n",
       " 'poor',\n",
       " 'high',\n",
       " 'making',\n",
       " 'leave',\n",
       " 'crime',\n",
       " 'african',\n",
       " 'working',\n",
       " 'may',\n",
       " 'video',\n",
       " 'parent',\n",
       " 'arab',\n",
       " 'course',\n",
       " 'baby',\n",
       " 'once',\n",
       " 'left',\n",
       " 'imagine',\n",
       " 'america',\n",
       " 'worst',\n",
       " 'looking',\n",
       " 'cunt',\n",
       " 'probably',\n",
       " 'mind',\n",
       " 'street',\n",
       " 'scum',\n",
       " 'normal',\n",
       " 'idiot',\n",
       " 'whole',\n",
       " 'talking',\n",
       " 'europe',\n",
       " 'dog',\n",
       " 'death',\n",
       " 'city',\n",
       " 'useless',\n",
       " 'suck',\n",
       " 'remember',\n",
       " 'already',\n",
       " 'maybe',\n",
       " 'far',\n",
       " 'control',\n",
       " 'terrorist',\n",
       " 'pay',\n",
       " 'seems',\n",
       " 'both',\n",
       " 'absolutely',\n",
       " 'police',\n",
       " 'name',\n",
       " 'act',\n",
       " 'honest',\n",
       " 'european',\n",
       " 'power',\n",
       " 'each',\n",
       " 'wear',\n",
       " 'term',\n",
       " 'stand',\n",
       " 'using',\n",
       " 'read',\n",
       " 'history',\n",
       " 'food',\n",
       " 'coming',\n",
       " 'become',\n",
       " 'able',\n",
       " 'male',\n",
       " 'hey',\n",
       " 'happy',\n",
       " 'lie',\n",
       " 'instead',\n",
       " 'either',\n",
       " 'class',\n",
       " 'bullshit',\n",
       " 'wonder',\n",
       " 'religion',\n",
       " 'nazi',\n",
       " 'islam',\n",
       " 'came',\n",
       " 'single',\n",
       " 'taking',\n",
       " 'covid',\n",
       " 'calling',\n",
       " 'born',\n",
       " 'area',\n",
       " 'went',\n",
       " 'watch',\n",
       " 'found',\n",
       " 'etc',\n",
       " 'turn',\n",
       " 'monkey',\n",
       " 'mixed',\n",
       " 'million',\n",
       " 'head',\n",
       " 'fucked',\n",
       " 'whatever',\n",
       " 'truth',\n",
       " 'racism',\n",
       " 'question',\n",
       " 'ok',\n",
       " 'honestly',\n",
       " 'completely',\n",
       " 'body',\n",
       " 'africa',\n",
       " 'young',\n",
       " 'under',\n",
       " 'rather',\n",
       " 'politician',\n",
       " 'fight',\n",
       " 'dumb',\n",
       " 'agree',\n",
       " 'trash',\n",
       " 'sorry',\n",
       " 'exist',\n",
       " 'case',\n",
       " 'run',\n",
       " 'rest',\n",
       " 'number',\n",
       " 'nation',\n",
       " 'eat',\n",
       " 'damn',\n",
       " 'week',\n",
       " 'simply',\n",
       " 'rape',\n",
       " 'foreigner',\n",
       " 'dude',\n",
       " 'disabled',\n",
       " 'thanks',\n",
       " 'quite',\n",
       " 'myself',\n",
       " 'mental',\n",
       " 'definitely',\n",
       " '2',\n",
       " 'english',\n",
       " 'wife',\n",
       " 'especially',\n",
       " 'criminal',\n",
       " 'christian',\n",
       " 'car',\n",
       " 'attention',\n",
       " 'almost',\n",
       " 'whore',\n",
       " 'often',\n",
       " 'likely',\n",
       " 'hand',\n",
       " 'guess',\n",
       " 'comment',\n",
       " 'shitty',\n",
       " 'nigga',\n",
       " 'story',\n",
       " 'seem',\n",
       " 'sad',\n",
       " 'retard',\n",
       " 'seriously',\n",
       " 'dick',\n",
       " 'rule',\n",
       " 'proud',\n",
       " 'learn',\n",
       " 'hear',\n",
       " 'donât',\n",
       " 'stuff',\n",
       " 'speak',\n",
       " 'respect',\n",
       " 'move',\n",
       " 'gypsy',\n",
       " 'difference',\n",
       " 'view',\n",
       " 'social',\n",
       " 'sense',\n",
       " 'relationship',\n",
       " 'news',\n",
       " 'immigration',\n",
       " 'side',\n",
       " 'seeing',\n",
       " 'political',\n",
       " 'open',\n",
       " 'given',\n",
       " 'dead',\n",
       " 'bit',\n",
       " 'happen',\n",
       " 'claim',\n",
       " 'blame',\n",
       " 'ask',\n",
       " 'wanted',\n",
       " 'type',\n",
       " 'started',\n",
       " 'sexual',\n",
       " 'night',\n",
       " 'attack',\n",
       " 'smart',\n",
       " 'public',\n",
       " 'piece',\n",
       " 'killed',\n",
       " 'evil',\n",
       " 'vote',\n",
       " 'value',\n",
       " 'level',\n",
       " 'heard',\n",
       " 'during',\n",
       " '3',\n",
       " 'thinking',\n",
       " 'slave',\n",
       " 'play',\n",
       " 'choice',\n",
       " 'china',\n",
       " 'son',\n",
       " 'rat',\n",
       " 'party',\n",
       " 'order',\n",
       " 'experience',\n",
       " 'ago',\n",
       " 'violent',\n",
       " 'thread',\n",
       " 'straight',\n",
       " 'obama',\n",
       " 'due',\n",
       " 'beautiful',\n",
       " 'shut',\n",
       " 'self',\n",
       " 'middle',\n",
       " 'land',\n",
       " 'exactly',\n",
       " 'age',\n",
       " 'wait',\n",
       " 'totally',\n",
       " 'sound',\n",
       " 'lazy',\n",
       " 'game',\n",
       " 'future',\n",
       " 'funny',\n",
       " 'couple',\n",
       " 'brain',\n",
       " '10',\n",
       " 'weird',\n",
       " 'skin',\n",
       " 'ireland',\n",
       " 'common',\n",
       " 'tv',\n",
       " 'together',\n",
       " 's',\n",
       " 'r',\n",
       " 'nature',\n",
       " 'destroy',\n",
       " 'belong',\n",
       " 'based',\n",
       " 'worth',\n",
       " 'violence',\n",
       " 'towards',\n",
       " 'shot',\n",
       " 'month',\n",
       " 'homosexual',\n",
       " 'example',\n",
       " 'welcome',\n",
       " 'rich',\n",
       " 'mask',\n",
       " 'horrible',\n",
       " 'everywhere',\n",
       " 'benefit',\n",
       " 'system',\n",
       " 'specie',\n",
       " 'language',\n",
       " 'feeling',\n",
       " 'dress',\n",
       " 'absolute',\n",
       " 'past',\n",
       " 'marriage',\n",
       " 'deal',\n",
       " 'citizen',\n",
       " 'buy',\n",
       " 'murder',\n",
       " 'met',\n",
       " 'massive',\n",
       " 'crazy',\n",
       " 'clearly',\n",
       " 'awful',\n",
       " 'anyway',\n",
       " 'virus',\n",
       " 'near',\n",
       " 'n',\n",
       " 'mexican',\n",
       " 'longer',\n",
       " 'bunch',\n",
       " 'brother',\n",
       " 'asylum',\n",
       " '1',\n",
       " 'sort',\n",
       " 'picture',\n",
       " 'joke',\n",
       " 'gone',\n",
       " 'crap',\n",
       " 'accept',\n",
       " 'whether',\n",
       " 'okay',\n",
       " 'majority',\n",
       " 'eye',\n",
       " 'equal',\n",
       " 'earth',\n",
       " 'business',\n",
       " 'basically',\n",
       " 'usually',\n",
       " 'tranny',\n",
       " 'thank',\n",
       " 'sake',\n",
       " 'possible',\n",
       " 'nike',\n",
       " 'mother',\n",
       " 'modern',\n",
       " 'important',\n",
       " 'hot',\n",
       " 'haha',\n",
       " 'east',\n",
       " 'drug',\n",
       " 'daughter',\n",
       " 'dark',\n",
       " 'conservative',\n",
       " 'brown',\n",
       " 'britain',\n",
       " 'anymore',\n",
       " 'short',\n",
       " 'reality',\n",
       " 'illegal',\n",
       " 'bloody',\n",
       " 'alone',\n",
       " 'actual',\n",
       " 'sometimes',\n",
       " 'rate',\n",
       " 'mate',\n",
       " 'happens',\n",
       " 'fun',\n",
       " 'easy',\n",
       " 'abuse',\n",
       " '100',\n",
       " 'wearing',\n",
       " 'telling',\n",
       " 'reddit',\n",
       " 'racial',\n",
       " 'pussy',\n",
       " 'low',\n",
       " 'justin',\n",
       " 'individual',\n",
       " 'fag',\n",
       " 'entire',\n",
       " 'chink',\n",
       " 'break',\n",
       " 'action',\n",
       " 'wow',\n",
       " 'trust',\n",
       " 'tried',\n",
       " 'taken',\n",
       " 'non',\n",
       " 'half',\n",
       " 'hair',\n",
       " 'date',\n",
       " 'victim',\n",
       " 'situation',\n",
       " 'plan',\n",
       " 'neighbour',\n",
       " 'e',\n",
       " 'bring',\n",
       " 'tax',\n",
       " 'rid',\n",
       " 'obviously',\n",
       " 'marry',\n",
       " 'local',\n",
       " 'knew',\n",
       " 'husband',\n",
       " 'forget',\n",
       " 'fake',\n",
       " 'dangerous',\n",
       " 'cry',\n",
       " 'brexit',\n",
       " 'amount',\n",
       " 'win',\n",
       " 'save',\n",
       " 'rapist',\n",
       " 'priest',\n",
       " 'minority',\n",
       " 'indigenous',\n",
       " 'follow',\n",
       " 'finally',\n",
       " 'fault',\n",
       " 'fair',\n",
       " 'cool',\n",
       " 'clear',\n",
       " 'answer',\n",
       " 'western',\n",
       " 'west',\n",
       " 'top',\n",
       " 'took',\n",
       " 'sister',\n",
       " 'married',\n",
       " 'killing',\n",
       " 'iq',\n",
       " 'ignorant',\n",
       " 'fine',\n",
       " 'fear',\n",
       " 'extremely',\n",
       " 'expect',\n",
       " 'book',\n",
       " 'banned',\n",
       " 'angry',\n",
       " 'within',\n",
       " 'truly',\n",
       " 'town',\n",
       " 'threat',\n",
       " 'president',\n",
       " 'pakistani',\n",
       " 'listen',\n",
       " 'general',\n",
       " 'fat',\n",
       " 'belief',\n",
       " 'apparently',\n",
       " 'amazing',\n",
       " 'treated',\n",
       " 'transgender',\n",
       " 'pig',\n",
       " 'otherwise',\n",
       " 'native',\n",
       " 'mostly',\n",
       " 'known',\n",
       " 'glad',\n",
       " 'certain',\n",
       " 'camel',\n",
       " 'behind',\n",
       " 'among',\n",
       " 'twat',\n",
       " 'tory',\n",
       " 'terrible',\n",
       " 'small',\n",
       " 'share',\n",
       " 'shame',\n",
       " 'quality',\n",
       " 'pain',\n",
       " 'line',\n",
       " 'large',\n",
       " 'kike',\n",
       " 'health',\n",
       " 'happened',\n",
       " 'fit',\n",
       " 'behavior',\n",
       " 'beat',\n",
       " '5',\n",
       " 'watching',\n",
       " 'soon',\n",
       " 'safe',\n",
       " 'ruin',\n",
       " 'moment',\n",
       " 'join',\n",
       " 'hurt',\n",
       " 'huge',\n",
       " 'germany',\n",
       " 'german',\n",
       " 'enjoy',\n",
       " 'despite',\n",
       " 'consider',\n",
       " 'cat',\n",
       " 'wtf',\n",
       " 'surprised',\n",
       " 'rubbish',\n",
       " 'loser',\n",
       " 'hour',\n",
       " 'doctor',\n",
       " 'argument',\n",
       " 'allow',\n",
       " 'nobody',\n",
       " 'itâs',\n",
       " 'hit',\n",
       " 'ethnic',\n",
       " 'cop',\n",
       " 'colour',\n",
       " 'close',\n",
       " 'border',\n",
       " 'associated',\n",
       " 'unless',\n",
       " 'playing',\n",
       " 'mass',\n",
       " 'mad',\n",
       " 'lgbt',\n",
       " 'including',\n",
       " 'homo',\n",
       " 'himself',\n",
       " 'filthy',\n",
       " 'drink',\n",
       " 'dad',\n",
       " 'cut',\n",
       " 'check',\n",
       " 'century',\n",
       " 'burn',\n",
       " 'ball',\n",
       " 'three',\n",
       " 'strong',\n",
       " 'speech',\n",
       " 'set',\n",
       " 'service',\n",
       " 'serious',\n",
       " 'phone',\n",
       " 'natural',\n",
       " 'lack',\n",
       " 'inferior',\n",
       " 'identity',\n",
       " 'gross',\n",
       " 'form',\n",
       " 'force',\n",
       " 'father',\n",
       " 'economy',\n",
       " 'corona',\n",
       " 'complete',\n",
       " 'charge',\n",
       " 'blood',\n",
       " 'average',\n",
       " 'tend',\n",
       " 'team',\n",
       " 'smell',\n",
       " 'similar',\n",
       " 'period',\n",
       " 'online',\n",
       " 'mum',\n",
       " 'lose',\n",
       " 'india',\n",
       " 'except',\n",
       " 'created',\n",
       " 'sport',\n",
       " 'slavery',\n",
       " 'second',\n",
       " 'policy',\n",
       " 'photo',\n",
       " 'pas',\n",
       " 'london',\n",
       " 'israel',\n",
       " 'hitler',\n",
       " 'decent',\n",
       " 'clean',\n",
       " 'canada',\n",
       " 'blm',\n",
       " 'asshole',\n",
       " 'along',\n",
       " 'afraid',\n",
       " 'across',\n",
       " 'turned',\n",
       " 'study',\n",
       " 'slut',\n",
       " 'sleep',\n",
       " 'shop',\n",
       " 'result',\n",
       " 'negro',\n",
       " 'music',\n",
       " 'morning',\n",
       " 'meet',\n",
       " 'folk',\n",
       " 'explain',\n",
       " 'dating',\n",
       " 'ape',\n",
       " 'treat',\n",
       " 'slur',\n",
       " 'red',\n",
       " 'realise',\n",
       " 'positive',\n",
       " 'planet',\n",
       " 'personal',\n",
       " 'lost',\n",
       " 'lady',\n",
       " 'giving',\n",
       " 'girlfriend',\n",
       " 'died',\n",
       " 'account',\n",
       " '6',\n",
       " 'usa',\n",
       " 'therefore',\n",
       " 'super',\n",
       " 'step',\n",
       " 'movie',\n",
       " 'light',\n",
       " 'legal',\n",
       " 'jesus',\n",
       " 'freedom',\n",
       " 'diversity',\n",
       " 'create',\n",
       " 'considered',\n",
       " 'computer',\n",
       " 'cockroach',\n",
       " 'biggest',\n",
       " 'behaviour',\n",
       " 'united',\n",
       " 'unfortunately',\n",
       " 'tired',\n",
       " 'terrorism',\n",
       " 'seeker',\n",
       " 'perfect',\n",
       " 'partner',\n",
       " 'outside',\n",
       " 'lying',\n",
       " 'inside',\n",
       " 'grow',\n",
       " 'generation',\n",
       " 'forced',\n",
       " 'asked',\n",
       " 'waste',\n",
       " 'walk',\n",
       " 'thousand',\n",
       " 'send',\n",
       " 'national',\n",
       " 'innocent',\n",
       " 'false',\n",
       " 'current',\n",
       " 'continue',\n",
       " 'civilization',\n",
       " 'bisexual',\n",
       " 'worked',\n",
       " 'teacher',\n",
       " 'teach',\n",
       " 'supremacist',\n",
       " 'steal',\n",
       " 'south',\n",
       " 'somehow',\n",
       " 'scared',\n",
       " 'role',\n",
       " 'pregnant',\n",
       " 'obvious',\n",
       " 'nasty',\n",
       " 'moral',\n",
       " 'looked',\n",
       " 'lmao',\n",
       " 'letting',\n",
       " 'lead',\n",
       " 'ill',\n",
       " 'france',\n",
       " 'fall',\n",
       " 'eu',\n",
       " 'donald',\n",
       " 'coronavirus',\n",
       " 'wild',\n",
       " 'welfare',\n",
       " 'upon',\n",
       " 'twitter',\n",
       " 'traveller',\n",
       " 'total',\n",
       " 'throw',\n",
       " 'queer',\n",
       " 'pure',\n",
       " 'position',\n",
       " 'peace',\n",
       " 'opposite',\n",
       " 'office',\n",
       " 'nonsense',\n",
       " 'moron',\n",
       " 'mistake',\n",
       " 'migrant',\n",
       " 'load',\n",
       " 'japanese',\n",
       " 'iâm',\n",
       " 'islamic',\n",
       " 'illness',\n",
       " 'foreign',\n",
       " 'difficult',\n",
       " 'degenerate',\n",
       " 'breed',\n",
       " 'boris',\n",
       " 'bieber',\n",
       " 'b',\n",
       " 'annoying',\n",
       " 'water',\n",
       " 'special',\n",
       " 'russian',\n",
       " 'pandemic',\n",
       " 'freak',\n",
       " 'fighting',\n",
       " 'fan',\n",
       " 'early',\n",
       " 'door',\n",
       " 'disagree',\n",
       " 'decision',\n",
       " 'compared',\n",
       " 'clothes',\n",
       " 'choose',\n",
       " 'character',\n",
       " 'chance',\n",
       " 'cancer',\n",
       " 'became',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7fab03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   4,   47,    5,  487,  326,  888, 1166,    6,  398,  772,    1,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0], dtype=int64),\n",
       " 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "dataset = tensorflow.data.Dataset.from_tensor_slices((vectorized_text, y))\n",
    "dataset = dataset.shuffle(18000)\n",
    "dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a07f090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text     damn i thought they had strict gun law in germany\n",
       "Label                                                    0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a68421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
