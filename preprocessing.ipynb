{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting hate speech\n",
    "\n",
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess input text by tokenizing, removing stopwords,\n",
    "    and lemmatizing.\n",
    "    \"\"\"\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize tokens\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# Example usage\n",
    "example_text = \"This is an example sentence to demonstrate the preprocessing.\"\n",
    "processed_example = preprocess_text(example_text)\n",
    "print(processed_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "dataset = pd.read_csv('labeled_dataset.csv')\n",
    "print(dataset.head())\n",
    "\n",
    "# Remove duplicate rows\n",
    "dataset_unique = dataset.drop_duplicates()\n",
    "\n",
    "# Handle missing values by dropping rows with any missing data\n",
    "dataset_clean = dataset_unique.dropna()\n",
    "\n",
    "# Drop the 'count' column from the dataset\n",
    "dataset_final = dataset_clean.drop(columns=['count'])\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(dataset_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    text = input_text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'&', 'and', text)  # Replace '&' with 'and'\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmanization\n",
    "remove stop words\n",
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply text normalization\n",
    "dataset_final['text_clean'] = dataset_final['text'].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "dataset_final['text_tokens'] = dataset_final['text_clean'].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "dataset_final['text_tokens'] = dataset_final['text_tokens'].apply(lambda tokens: [token for token in tokens if token not in stopword_list])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dataset_final['text_tokens'] = dataset_final['text_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "# Display the processed dataset\n",
    "print(dataset_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select and reorder relevant columns\n",
    "dataset_final = dataset_final[['hate_speech', 'offensive_language', 'neither', 'class', 'text_clean']]\n",
    "print(dataset_final.head())\n",
    "\n",
    "# Save the cleaned dataset to a CSV file\n",
    "dataset_final.to_csv('cleaned_dataset.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
