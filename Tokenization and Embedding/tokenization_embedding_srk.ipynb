{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "702f1c06",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, subwords, or even characters, depending on the tokenization strategy used. Tokenization is a fundamental step as it converts raw text into a format that can be more easily analyzed and processed by machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8106349",
   "metadata": {},
   "source": [
    "Word Tokenization:\n",
    "word_tokenize is a function in Python that splits a given sentence into words using the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e6e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"HateSpeechDetection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc9812d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\balus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['Tokens']=data['Text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8625317b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Damn, I, thought, they, had, strict, gun, law...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dont care about what it stands for or anythi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, dont, care, about, what, it, stands, for, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's not a group it's an idea lol</td>\n",
       "      <td>0</td>\n",
       "      <td>[It, 's, not, a, group, it, 's, an, idea, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So it's not just America!</td>\n",
       "      <td>0</td>\n",
       "      <td>[So, it, 's, not, just, America, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "      <td>[The, dog, is, a, spectacular, dancer, conside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>I find rats nicer and cleaner than most Chinese</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, find, rats, nicer, and, cleaner, than, mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>Check out this niggar, they hit things like wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Check, out, this, niggar, ,, they, hit, thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become an absolute shambles, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, country, has, become, an, absolute, sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>Me aged 16 = anti-Semitism is bad Me aged 18 =...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Me, aged, 16, =, anti-Semitism, is, bad, Me, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>so messed up saying blacks don't deserve rights</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, messed, up, saying, blacks, do, n't, dese...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label  \\\n",
       "0      Damn I thought they had strict gun laws in Ger...      0   \n",
       "1      I dont care about what it stands for or anythi...      0   \n",
       "2                      It's not a group it's an idea lol      0   \n",
       "3                              So it's not just America!      0   \n",
       "4      The dog is a spectacular dancer considering he...      0   \n",
       "...                                                  ...    ...   \n",
       "17591    I find rats nicer and cleaner than most Chinese      1   \n",
       "17592  Check out this niggar, they hit things like wi...      1   \n",
       "17593  this country has become an absolute shambles, ...      0   \n",
       "17594  Me aged 16 = anti-Semitism is bad Me aged 18 =...      1   \n",
       "17595    so messed up saying blacks don't deserve rights      0   \n",
       "\n",
       "                                                  Tokens  \n",
       "0      [Damn, I, thought, they, had, strict, gun, law...  \n",
       "1      [I, dont, care, about, what, it, stands, for, ...  \n",
       "2         [It, 's, not, a, group, it, 's, an, idea, lol]  \n",
       "3                    [So, it, 's, not, just, America, !]  \n",
       "4      [The, dog, is, a, spectacular, dancer, conside...  \n",
       "...                                                  ...  \n",
       "17591  [I, find, rats, nicer, and, cleaner, than, mos...  \n",
       "17592  [Check, out, this, niggar, ,, they, hit, thing...  \n",
       "17593  [this, country, has, become, an, absolute, sha...  \n",
       "17594  [Me, aged, 16, =, anti-Semitism, is, bad, Me, ...  \n",
       "17595  [so, messed, up, saying, blacks, do, n't, dese...  \n",
       "\n",
       "[17596 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ab1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Text'].values\n",
    "labels = data['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ca387",
   "metadata": {},
   "source": [
    "# Embedding:\n",
    "Embedding in the context of deep learning and natural language processing (NLP) is a way of representing words or phrases as dense vectors in a continuous vector space. These vectors capture semantic meanings and relationships between words. Embeddings transform the sparse, high-dimensional data of words into a lower-dimensional space, where similar words have similar vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043e945",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Word2Vec transforms each word into a dense vector of fixed size. It captures semantic meanings by training on large text corpora. Word2Vec creates vectors of the words that are distributed numerical representations of word features – these word features could comprise of words that represent the context of the individual words present in our vocabulary.\n",
    "\n",
    "Two different model architectures that can be used by Word2Vec to create the word embeddings are the Continuous Bag of Words (CBOW) model & the Skip-Gram model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a60dee",
   "metadata": {},
   "source": [
    "# Skip-Gram\n",
    "Predicts context words based on a target word. Trains the model to maximize the probability of context words given a target word.\n",
    "\n",
    "Pros: Effective in capturing syntactic and semantic relationships, works well with small datasets.\n",
    "\n",
    "Cons: Computationally more expensive than CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "578b1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17109032  0.033767   -0.18476258 ... -0.10014948  0.10881008\n",
      "  -0.11326653]\n",
      " [ 0.22616751  0.03877444 -0.25352594 ... -0.07180958  0.15895079\n",
      "  -0.16575725]\n",
      " [ 0.1826731   0.00148701 -0.18673538 ... -0.11800708  0.13921864\n",
      "  -0.17711399]\n",
      " ...\n",
      " [ 0.17524712 -0.0284111  -0.1633537  ... -0.10352435  0.1074788\n",
      "  -0.13104244]\n",
      " [ 0.17381686  0.02411527 -0.19371334 ... -0.06752601  0.15991086\n",
      "  -0.16255414]\n",
      " [ 0.18425429  0.02087011 -0.19497301 ... -0.12127761  0.12777068\n",
      "  -0.16926508]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec_embedding_sg(texts):\n",
    "    model = Word2Vec(texts, vector_size=200, window=6, min_count=1, workers=4,sg=1)\n",
    "    word_vectors = model.wv\n",
    "    #print(word_vectors)\n",
    "\n",
    "    def get_word2vec_embeddings(text, word_vectors):\n",
    "        embeddings = [word_vectors[word] for word in text if word in word_vectors]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(200)\n",
    "\n",
    "    embeddings = np.array([get_word2vec_embeddings(text, word_vectors) for text in texts])\n",
    "    return embeddings\n",
    "\n",
    "embeddings_w2v_sg = word2vec_embedding_sg(data['Tokens'])\n",
    "print(embeddings_w2v_sg)\n",
    "X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2 = train_test_split(embeddings_w2v_sg, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9c60f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_evaluate_rf(X_train_emb, X_test_emb, y_train, y_test):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train_emb, y_train)\n",
    "    y_pred = rf.predict(X_test_emb)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9382041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Accuracy: 0.6551136363636364\n",
      "Word2Vec Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.84      0.74      2094\n",
      "           1       0.62      0.38      0.47      1426\n",
      "\n",
      "    accuracy                           0.66      3520\n",
      "   macro avg       0.64      0.61      0.61      3520\n",
      "weighted avg       0.65      0.66      0.63      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_w2v2, report_w2v2 = train_evaluate_rf(X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2)\n",
    "print(f'Word2Vec skip-gram Accuracy: {accuracy_w2v2}')\n",
    "print(f'Word2Vec skip-gram Classification Report:\\n{report_w2v2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a17c05",
   "metadata": {},
   "source": [
    "\n",
    "# Word2Vec skip-gram observations:\n",
    "                            vector_size     window_size     Accuracy\n",
    "                                50              3              63.8\n",
    "                                100             3              65.3\n",
    "                                100             4              64.3\n",
    "                                150             4              64.0\n",
    "                                150             5              65.1\n",
    "                                200             5              64.2\n",
    "                                200             6              67.9\n",
    "                                250             6              66.7\n",
    "                                300             6              65.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
