{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "702f1c06",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, subwords, or even characters, depending on the tokenization strategy used. Tokenization is a fundamental step as it converts raw text into a format that can be more easily analyzed and processed by machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8106349",
   "metadata": {},
   "source": [
    "Word Tokenization:\n",
    "word_tokenize is a function in Python that splits a given sentence into words using the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bc9812d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\balui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['Tokens']=data['Text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71943cd",
   "metadata": {},
   "source": [
    "Rule-Based Tokenization: This uses predefined rules to handle complex cases like contractions, hyphenated words, and special characters. (e.g., Penn Treebank Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34124df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "def rule_based_tokenize(text):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "data['token_rbt']=data['Text'].apply(rule_based_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6f673",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE): This subword tokenization method iteratively merges the most frequent character pairs to create a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edebef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\balui\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\balui\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def bpe_tokenize(text):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    return tokenizer.tokenize(text)\n",
    "data['token_bpe']=data['Text'].apply(rule_based_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8625317b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>token_rbt</th>\n",
       "      <th>token_bpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damn thought they had strict gun law germany</td>\n",
       "      <td>0</td>\n",
       "      <td>[damn, thought, they, had, strict, gun, law, g...</td>\n",
       "      <td>[damn, thought, they, had, strict, gun, law, g...</td>\n",
       "      <td>[damn, thought, they, had, strict, gun, law, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not care about what stand for anything its con...</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, care, about, what, stand, for, anything,...</td>\n",
       "      <td>[not, care, about, what, stand, for, anything,...</td>\n",
       "      <td>[not, care, about, what, stand, for, anything,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not group idea lol</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, group, idea, lol]</td>\n",
       "      <td>[not, group, idea, lol]</td>\n",
       "      <td>[not, group, idea, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not just america</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, just, america]</td>\n",
       "      <td>[not, just, america]</td>\n",
       "      <td>[not, just, america]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the dog spectacular dancer considering has two...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, dog, spectacular, dancer, considering, h...</td>\n",
       "      <td>[the, dog, spectacular, dancer, considering, h...</td>\n",
       "      <td>[the, dog, spectacular, dancer, considering, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>find rat nicer and cleaner than most chinese</td>\n",
       "      <td>1</td>\n",
       "      <td>[find, rat, nicer, and, cleaner, than, most, c...</td>\n",
       "      <td>[find, rat, nicer, and, cleaner, than, most, c...</td>\n",
       "      <td>[find, rat, nicer, and, cleaner, than, most, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>check out this niggar they hit thing like wild...</td>\n",
       "      <td>1</td>\n",
       "      <td>[check, out, this, niggar, they, hit, thing, l...</td>\n",
       "      <td>[check, out, this, niggar, they, hit, thing, l...</td>\n",
       "      <td>[check, out, this, niggar, they, hit, thing, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become absolute shamble the a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, country, has, become, absolute, shamble...</td>\n",
       "      <td>[this, country, has, become, absolute, shamble...</td>\n",
       "      <td>[this, country, has, become, absolute, shamble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>aged 16 antisemitism bad aged 18 antisemitism ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[aged, 16, antisemitism, bad, aged, 18, antise...</td>\n",
       "      <td>[aged, 16, antisemitism, bad, aged, 18, antise...</td>\n",
       "      <td>[aged, 16, antisemitism, bad, aged, 18, antise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>messed saying black not deserve right</td>\n",
       "      <td>0</td>\n",
       "      <td>[messed, saying, black, not, deserve, right]</td>\n",
       "      <td>[messed, saying, black, not, deserve, right]</td>\n",
       "      <td>[messed, saying, black, not, deserve, right]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label  \\\n",
       "0           damn thought they had strict gun law germany      0   \n",
       "1      not care about what stand for anything its con...      0   \n",
       "2                                     not group idea lol      0   \n",
       "3                                       not just america      0   \n",
       "4      the dog spectacular dancer considering has two...      0   \n",
       "...                                                  ...    ...   \n",
       "17591       find rat nicer and cleaner than most chinese      1   \n",
       "17592  check out this niggar they hit thing like wild...      1   \n",
       "17593  this country has become absolute shamble the a...      0   \n",
       "17594  aged 16 antisemitism bad aged 18 antisemitism ...      1   \n",
       "17595              messed saying black not deserve right      0   \n",
       "\n",
       "                                                  Tokens  \\\n",
       "0      [damn, thought, they, had, strict, gun, law, g...   \n",
       "1      [not, care, about, what, stand, for, anything,...   \n",
       "2                                [not, group, idea, lol]   \n",
       "3                                   [not, just, america]   \n",
       "4      [the, dog, spectacular, dancer, considering, h...   \n",
       "...                                                  ...   \n",
       "17591  [find, rat, nicer, and, cleaner, than, most, c...   \n",
       "17592  [check, out, this, niggar, they, hit, thing, l...   \n",
       "17593  [this, country, has, become, absolute, shamble...   \n",
       "17594  [aged, 16, antisemitism, bad, aged, 18, antise...   \n",
       "17595       [messed, saying, black, not, deserve, right]   \n",
       "\n",
       "                                               token_rbt  \\\n",
       "0      [damn, thought, they, had, strict, gun, law, g...   \n",
       "1      [not, care, about, what, stand, for, anything,...   \n",
       "2                                [not, group, idea, lol]   \n",
       "3                                   [not, just, america]   \n",
       "4      [the, dog, spectacular, dancer, considering, h...   \n",
       "...                                                  ...   \n",
       "17591  [find, rat, nicer, and, cleaner, than, most, c...   \n",
       "17592  [check, out, this, niggar, they, hit, thing, l...   \n",
       "17593  [this, country, has, become, absolute, shamble...   \n",
       "17594  [aged, 16, antisemitism, bad, aged, 18, antise...   \n",
       "17595       [messed, saying, black, not, deserve, right]   \n",
       "\n",
       "                                               token_bpe  \n",
       "0      [damn, thought, they, had, strict, gun, law, g...  \n",
       "1      [not, care, about, what, stand, for, anything,...  \n",
       "2                                [not, group, idea, lol]  \n",
       "3                                   [not, just, america]  \n",
       "4      [the, dog, spectacular, dancer, considering, h...  \n",
       "...                                                  ...  \n",
       "17591  [find, rat, nicer, and, cleaner, than, most, c...  \n",
       "17592  [check, out, this, niggar, they, hit, thing, l...  \n",
       "17593  [this, country, has, become, absolute, shamble...  \n",
       "17594  [aged, 16, antisemitism, bad, aged, 18, antise...  \n",
       "17595       [messed, saying, black, not, deserve, right]  \n",
       "\n",
       "[17596 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ca387",
   "metadata": {},
   "source": [
    "# Embedding:\n",
    "Embedding in the context of deep learning and natural language processing (NLP) is a way of representing words or phrases as dense vectors in a continuous vector space. These vectors capture semantic meanings and relationships between words. Embeddings transform the sparse, high-dimensional data of words into a lower-dimensional space, where similar words have similar vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942c0c7",
   "metadata": {},
   "source": [
    "# Count based embeddings (Non-context)\n",
    "Non-contextual embeddings assign a single vector representation to each word, regardless of its context in a sentence. These embeddings are static and do not change based on the words surrounding them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a3f3e",
   "metadata": {},
   "source": [
    "# One Hot Encoding: \n",
    "Non-contextual embeddings assign a single vector representation to each word, regardless of its context in a sentence. These embeddings are static and do not change based on the words surrounding them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ddfcd",
   "metadata": {},
   "source": [
    "Pros: Simple to implement, no need for pre-training.\n",
    "\n",
    "Cons: High dimensionality, no information about word similarity or context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71ab1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Text'].values\n",
    "labels = data['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd38793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4315)\t1\n",
      "  (0, 17087)\t1\n",
      "  (0, 17033)\t1\n",
      "  (0, 7589)\t1\n",
      "  (0, 16345)\t1\n",
      "  (0, 7542)\t1\n",
      "  (0, 9660)\t1\n",
      "  (0, 7162)\t1\n",
      "  (1, 11708)\t1\n",
      "  (1, 2841)\t1\n",
      "  (1, 432)\t1\n",
      "  (1, 18571)\t1\n",
      "  (1, 16163)\t1\n",
      "  (1, 6681)\t1\n",
      "  (1, 1153)\t1\n",
      "  (1, 9044)\t1\n",
      "  (1, 3732)\t1\n",
      "  (1, 9903)\t1\n",
      "  (1, 16979)\t1\n",
      "  (1, 15319)\t1\n",
      "  (2, 11708)\t1\n",
      "  (2, 7492)\t1\n",
      "  (2, 8307)\t1\n",
      "  (2, 10035)\t1\n",
      "  (3, 11708)\t1\n",
      "  :\t:\n",
      "  (17594, 14863)\t1\n",
      "  (17594, 11962)\t1\n",
      "  (17594, 1655)\t1\n",
      "  (17594, 17080)\t1\n",
      "  (17594, 6028)\t1\n",
      "  (17594, 136)\t1\n",
      "  (17594, 67)\t1\n",
      "  (17594, 5843)\t1\n",
      "  (17594, 3671)\t1\n",
      "  (17594, 13871)\t1\n",
      "  (17594, 9153)\t1\n",
      "  (17594, 16205)\t1\n",
      "  (17594, 14867)\t1\n",
      "  (17594, 10620)\t1\n",
      "  (17594, 711)\t1\n",
      "  (17594, 84)\t1\n",
      "  (17594, 11347)\t1\n",
      "  (17594, 1122)\t1\n",
      "  (17594, 14409)\t1\n",
      "  (17595, 11708)\t1\n",
      "  (17595, 2122)\t1\n",
      "  (17595, 14463)\t1\n",
      "  (17595, 4683)\t1\n",
      "  (17595, 14867)\t1\n",
      "  (17595, 10648)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "def one_hot_encoding(texts):\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_one_hot = one_hot_encoding(texts)\n",
    "X_train_one_hot, X_test_one_hot, y_train_one_hot, y_test_one_hot = train_test_split(embeddings_one_hot, labels, test_size=0.2, random_state=42)\n",
    "print(embeddings_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833da12d",
   "metadata": {},
   "source": [
    "# Term Frequency: \n",
    "    Converts text data to term frequency vectors.\n",
    "Pros: Simple, captures basic information about word importance.\n",
    "\n",
    "Cons: High dimensionality, does not capture semantic meanings or context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f934c106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4315)\t1\n",
      "  (0, 17087)\t1\n",
      "  (0, 17033)\t1\n",
      "  (0, 7589)\t1\n",
      "  (0, 16345)\t1\n",
      "  (0, 7542)\t1\n",
      "  (0, 9660)\t1\n",
      "  (0, 7162)\t1\n",
      "  (1, 11708)\t1\n",
      "  (1, 2841)\t1\n",
      "  (1, 432)\t1\n",
      "  (1, 18571)\t1\n",
      "  (1, 16163)\t1\n",
      "  (1, 6681)\t1\n",
      "  (1, 1153)\t1\n",
      "  (1, 9044)\t1\n",
      "  (1, 3732)\t1\n",
      "  (1, 9903)\t1\n",
      "  (1, 16979)\t1\n",
      "  (1, 15319)\t1\n",
      "  (2, 11708)\t1\n",
      "  (2, 7492)\t1\n",
      "  (2, 8307)\t1\n",
      "  (2, 10035)\t1\n",
      "  (3, 11708)\t1\n",
      "  :\t:\n",
      "  (17594, 14863)\t1\n",
      "  (17594, 11962)\t1\n",
      "  (17594, 1655)\t1\n",
      "  (17594, 17080)\t1\n",
      "  (17594, 6028)\t1\n",
      "  (17594, 136)\t1\n",
      "  (17594, 67)\t1\n",
      "  (17594, 5843)\t1\n",
      "  (17594, 3671)\t1\n",
      "  (17594, 13871)\t1\n",
      "  (17594, 9153)\t1\n",
      "  (17594, 16205)\t1\n",
      "  (17594, 14867)\t1\n",
      "  (17594, 10620)\t1\n",
      "  (17594, 711)\t3\n",
      "  (17594, 84)\t1\n",
      "  (17594, 11347)\t1\n",
      "  (17594, 1122)\t4\n",
      "  (17594, 14409)\t1\n",
      "  (17595, 11708)\t1\n",
      "  (17595, 2122)\t1\n",
      "  (17595, 14463)\t1\n",
      "  (17595, 4683)\t1\n",
      "  (17595, 14867)\t1\n",
      "  (17595, 10648)\t1\n"
     ]
    }
   ],
   "source": [
    "def term_frequency_encoding(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "embeddings_tf = term_frequency_encoding(texts)\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(embeddings_tf, labels, test_size=0.2, random_state=42)\n",
    "print(embeddings_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1634e9",
   "metadata": {},
   "source": [
    "# TF-IDF:\n",
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set).\n",
    "\n",
    "Pros: Highlights important words in a document, reduces the influence of common words.\n",
    "\n",
    "Cons: Still high dimensional, static representation, limited in capturing semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f25f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7162)\t0.38655864797204015\n",
      "  (0, 9660)\t0.3369183517812025\n",
      "  (0, 7542)\t0.4085287030056687\n",
      "  (0, 16345)\t0.49396554919123437\n",
      "  (0, 7589)\t0.26594905432797417\n",
      "  (0, 17033)\t0.15796206623433257\n",
      "  (0, 17087)\t0.327463512592553\n",
      "  (0, 4315)\t0.35106624018460947\n",
      "  (1, 15319)\t0.46764828617011794\n",
      "  (1, 16979)\t0.10452774819368486\n",
      "  (1, 9903)\t0.18460642886856596\n",
      "  (1, 3732)\t0.49051943994950004\n",
      "  (1, 9044)\t0.2687361424930761\n",
      "  (1, 1153)\t0.2965908260733827\n",
      "  (1, 6681)\t0.1649905427170199\n",
      "  (1, 16163)\t0.3416575901596179\n",
      "  (1, 18571)\t0.2044196148320207\n",
      "  (1, 432)\t0.21152394159488963\n",
      "  (1, 2841)\t0.29866739942996134\n",
      "  (1, 11708)\t0.12578228405018932\n",
      "  (2, 10035)\t0.5587285254775806\n",
      "  (2, 8307)\t0.5796117172772546\n",
      "  (2, 7492)\t0.548976877964676\n",
      "  (2, 11708)\t0.22471555236057383\n",
      "  (3, 923)\t0.8296004517693428\n",
      "  :\t:\n",
      "  (17594, 67)\t0.16153022657761285\n",
      "  (17594, 136)\t0.14012531222980573\n",
      "  (17594, 6028)\t0.12021987545987466\n",
      "  (17594, 17080)\t0.08624109760557543\n",
      "  (17594, 1655)\t0.10065247197644427\n",
      "  (17594, 11962)\t0.14367759117952028\n",
      "  (17594, 14863)\t0.08369058678924071\n",
      "  (17594, 1143)\t0.08655009497914094\n",
      "  (17594, 1869)\t0.0744252023511236\n",
      "  (17594, 5126)\t0.08732671778893056\n",
      "  (17594, 7769)\t0.05742683107793596\n",
      "  (17594, 12145)\t0.07736679792811813\n",
      "  (17594, 7294)\t0.0868640719333243\n",
      "  (17594, 1285)\t0.04297862021060185\n",
      "  (17594, 19019)\t0.10121361073520913\n",
      "  (17594, 18571)\t0.07060850574509082\n",
      "  (17594, 432)\t0.07306240870084797\n",
      "  (17594, 2841)\t0.10316259917548003\n",
      "  (17594, 11708)\t0.13033924068323685\n",
      "  (17595, 10648)\t0.6316531031342569\n",
      "  (17595, 14867)\t0.4126509379813496\n",
      "  (17595, 4683)\t0.44771802006845846\n",
      "  (17595, 14463)\t0.34786010578750365\n",
      "  (17595, 2122)\t0.2825701551414922\n",
      "  (17595, 11708)\t0.17155054478134218\n",
      "(17596, 19209)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_embedding(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(texts)\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(embeddings_tfidf, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(embeddings_tfidf)\n",
    "print(embeddings_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6711c4",
   "metadata": {},
   "source": [
    "# Position based embeddings (context)\n",
    "Contextual embeddings generate different vector representations for a word based on its context within a sentence. These embeddings are dynamic and change depending on surrounding words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043e945",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Word2Vec transforms each word into a dense vector of fixed size. It captures semantic meanings by training on large text corpora. Word2Vec creates vectors of the words that are distributed numerical representations of word features â€“ these word features could comprise of words that represent the context of the individual words present in our vocabulary.\n",
    "\n",
    "Two different model architectures that can be used by Word2Vec to create the word embeddings are the Continuous Bag of Words (CBOW) model & the Skip-Gram model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91694afb",
   "metadata": {},
   "source": [
    "# CBOW\n",
    "In CBOW the words occurring in context (surrounding words) of a selected word are used as inputs and middle or selected word as the target.\n",
    "\n",
    "Pros: Efficient, captures syntactic and semantic relationships to some extent.\n",
    "\n",
    "Cons: Assumes context words are independent, which may not always hold true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "992a0744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02526417  0.17349869  0.00987022 ... -0.05650085  0.17430493\n",
      "  -0.15075786]\n",
      " [ 0.11437532  0.16749364 -0.00835051 ...  0.00312652  0.27611646\n",
      "  -0.24354184]\n",
      " [ 0.05128742  0.17605858 -0.07547641 ... -0.05376441  0.25279415\n",
      "  -0.28923002]\n",
      " ...\n",
      " [-0.02438427  0.2300602   0.04216736 ... -0.09735176  0.19842166\n",
      "  -0.10078315]\n",
      " [ 0.09299681  0.13884063 -0.04085092 ...  0.01131009  0.2328288\n",
      "  -0.22933097]\n",
      " [ 0.0789419   0.16689835 -0.11756516 ...  0.00448359  0.30474007\n",
      "  -0.3419309 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec_embedding_cbow(texts):\n",
    "    model = Word2Vec(texts, vector_size=300, window=5, min_count=1, workers=4,sg=0)\n",
    "    word_vectors = model.wv\n",
    "    #print(word_vectors)\n",
    "\n",
    "    def get_word2vec_embeddings(text, word_vectors):\n",
    "        embeddings = [word_vectors[word] for word in text if word in word_vectors]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(300)\n",
    "\n",
    "    embeddings = np.array([get_word2vec_embeddings(text, word_vectors) for text in texts])\n",
    "    return embeddings\n",
    "\n",
    "embeddings_w2v_cbow = word2vec_embedding_cbow(data['Tokens'])\n",
    "print(embeddings_w2v_cbow)\n",
    "X_train_w2v1, X_test_w2v1, y_train_w2v1, y_test_w2v1 = train_test_split(embeddings_w2v_cbow, labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a60dee",
   "metadata": {},
   "source": [
    "# Skip-Gram\n",
    "Predicts context words based on a target word. Trains the model to maximize the probability of context words given a target word.\n",
    "\n",
    "Pros: Effective in capturing syntactic and semantic relationships, works well with small datasets.\n",
    "\n",
    "Cons: Computationally more expensive than CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "578b1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17109032  0.033767   -0.18476258 ... -0.10014948  0.10881008\n",
      "  -0.11326653]\n",
      " [ 0.22616751  0.03877444 -0.25352594 ... -0.07180958  0.15895079\n",
      "  -0.16575725]\n",
      " [ 0.1826731   0.00148701 -0.18673538 ... -0.11800708  0.13921864\n",
      "  -0.17711399]\n",
      " ...\n",
      " [ 0.17524712 -0.0284111  -0.1633537  ... -0.10352435  0.1074788\n",
      "  -0.13104244]\n",
      " [ 0.17381686  0.02411527 -0.19371334 ... -0.06752601  0.15991086\n",
      "  -0.16255414]\n",
      " [ 0.18425429  0.02087011 -0.19497301 ... -0.12127761  0.12777068\n",
      "  -0.16926508]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec_embedding_sg(texts):\n",
    "    model = Word2Vec(texts, vector_size=200, window=6, min_count=1, workers=4,sg=1)\n",
    "    word_vectors = model.wv\n",
    "    #print(word_vectors)\n",
    "\n",
    "    def get_word2vec_embeddings(text, word_vectors):\n",
    "        embeddings = [word_vectors[word] for word in text if word in word_vectors]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(200)\n",
    "\n",
    "    embeddings = np.array([get_word2vec_embeddings(text, word_vectors) for text in texts])\n",
    "    return embeddings\n",
    "\n",
    "embeddings_w2v_sg = word2vec_embedding_sg(data['Tokens'])\n",
    "print(embeddings_w2v_sg)\n",
    "X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2 = train_test_split(embeddings_w2v_sg, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9c60f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_evaluate_rf(X_train_emb, X_test_emb, y_train, y_test):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train_emb, y_train)\n",
    "    y_pred = rf.predict(X_test_emb)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63001424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding Accuracy: 0.6724431818181819\n",
      "One Hot Encoding Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      2094\n",
      "           1       0.62      0.50      0.55      1426\n",
      "\n",
      "    accuracy                           0.67      3520\n",
      "   macro avg       0.66      0.64      0.65      3520\n",
      "weighted avg       0.67      0.67      0.66      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_one_hot, report_one_hot = train_evaluate_rf(X_train_one_hot, X_test_one_hot, y_train_one_hot, y_test_one_hot)\n",
    "print(f'One Hot Encoding Accuracy: {accuracy_one_hot}')\n",
    "print(f'One Hot Encoding Classification Report:\\n{report_one_hot}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "758aa191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Accuracy: 0.6764204545454545\n",
      "Term Frequency Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75      2094\n",
      "           1       0.63      0.50      0.56      1426\n",
      "\n",
      "    accuracy                           0.68      3520\n",
      "   macro avg       0.66      0.65      0.65      3520\n",
      "weighted avg       0.67      0.68      0.67      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_tf, report_tf = train_evaluate_rf(X_train_tf, X_test_tf, y_train_tf, y_test_tf)\n",
    "print(f'Term Frequency Accuracy: {accuracy_tf}')\n",
    "print(f'Term Frequency Classification Report:\\n{report_tf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6cf3564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.6849431818181818\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.83      0.76      2094\n",
      "           1       0.65      0.48      0.55      1426\n",
      "\n",
      "    accuracy                           0.68      3520\n",
      "   macro avg       0.68      0.65      0.65      3520\n",
      "weighted avg       0.68      0.68      0.67      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_tfidf, report_tfidf = train_evaluate_rf(X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf)\n",
    "print(f'TF-IDF Accuracy: {accuracy_tfidf}')\n",
    "print(f'TF-IDF Classification Report:\\n{report_tfidf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ad37243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Accuracy: 0.625\n",
      "Word2Vec Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72      2094\n",
      "           1       0.56      0.35      0.43      1426\n",
      "\n",
      "    accuracy                           0.62      3520\n",
      "   macro avg       0.60      0.58      0.58      3520\n",
      "weighted avg       0.61      0.62      0.60      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_w2v1, report_w2v1 = train_evaluate_rf(X_train_w2v1, X_test_w2v1, y_train_w2v1, y_test_w2v1)\n",
    "print(f'Word2Vec CBOW Accuracy: {accuracy_w2v1}')\n",
    "print(f'Word2Vec CBOW Classification Report:\\n{report_w2v1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb3de3",
   "metadata": {},
   "source": [
    "# Word2Vec CBOW observations:\n",
    "                            vector_size     window_size     Accuracy\n",
    "                                50              3              62.5\n",
    "                                100             3              62.9\n",
    "                                100             4              63.4\n",
    "                                150             4              62.5\n",
    "                                200             4              62.4\n",
    "                                200             5              62.4\n",
    "                                250             5              62.6\n",
    "                                300             5              62.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9382041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Accuracy: 0.6551136363636364\n",
      "Word2Vec Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.84      0.74      2094\n",
      "           1       0.62      0.38      0.47      1426\n",
      "\n",
      "    accuracy                           0.66      3520\n",
      "   macro avg       0.64      0.61      0.61      3520\n",
      "weighted avg       0.65      0.66      0.63      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_w2v2, report_w2v2 = train_evaluate_rf(X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2)\n",
    "print(f'Word2Vec skip-gram Accuracy: {accuracy_w2v2}')\n",
    "print(f'Word2Vec skip-gram Classification Report:\\n{report_w2v2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a17c05",
   "metadata": {},
   "source": [
    "\n",
    "# Word2Vec skip-gram observations:\n",
    "                            vector_size     window_size     Accuracy\n",
    "                                50              3              63.8\n",
    "                                100             3              65.3\n",
    "                                100             4              64.3\n",
    "                                150             4              64.0\n",
    "                                150             5              65.1\n",
    "                                200             5              64.2\n",
    "                                200             6              67.9\n",
    "                                250             6              66.7\n",
    "                                300             6              65.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
