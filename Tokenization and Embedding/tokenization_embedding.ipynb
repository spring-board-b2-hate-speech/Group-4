{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c08d094a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\balui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenizer(text):\n",
    "    return word_tokenize(text)\n",
    "data['Tokens']=data['Text'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f3a3ae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damn thought they had strict gun law germany</td>\n",
       "      <td>0</td>\n",
       "      <td>[damn, thought, they, had, strict, gun, law, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not care about what stand for anything its con...</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, care, about, what, stand, for, anything,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not group idea lol</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, group, idea, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not just america</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, just, america]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the dog spectacular dancer considering has two...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, dog, spectacular, dancer, considering, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>find rat nicer and cleaner than most chinese</td>\n",
       "      <td>1</td>\n",
       "      <td>[find, rat, nicer, and, cleaner, than, most, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>check out this niggar they hit thing like wild...</td>\n",
       "      <td>1</td>\n",
       "      <td>[check, out, this, niggar, they, hit, thing, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become absolute shamble the a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, country, has, become, absolute, shamble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>aged 16 antisemitism bad aged 18 antisemitism ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[aged, 16, antisemitism, bad, aged, 18, antise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>messed saying black not deserve right</td>\n",
       "      <td>0</td>\n",
       "      <td>[messed, saying, black, not, deserve, right]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label  \\\n",
       "0           damn thought they had strict gun law germany      0   \n",
       "1      not care about what stand for anything its con...      0   \n",
       "2                                     not group idea lol      0   \n",
       "3                                       not just america      0   \n",
       "4      the dog spectacular dancer considering has two...      0   \n",
       "...                                                  ...    ...   \n",
       "17591       find rat nicer and cleaner than most chinese      1   \n",
       "17592  check out this niggar they hit thing like wild...      1   \n",
       "17593  this country has become absolute shamble the a...      0   \n",
       "17594  aged 16 antisemitism bad aged 18 antisemitism ...      1   \n",
       "17595              messed saying black not deserve right      0   \n",
       "\n",
       "                                                  Tokens  \n",
       "0      [damn, thought, they, had, strict, gun, law, g...  \n",
       "1      [not, care, about, what, stand, for, anything,...  \n",
       "2                                [not, group, idea, lol]  \n",
       "3                                   [not, just, america]  \n",
       "4      [the, dog, spectacular, dancer, considering, h...  \n",
       "...                                                  ...  \n",
       "17591  [find, rat, nicer, and, cleaner, than, most, c...  \n",
       "17592  [check, out, this, niggar, they, hit, thing, l...  \n",
       "17593  [this, country, has, become, absolute, shamble...  \n",
       "17594  [aged, 16, antisemitism, bad, aged, 18, antise...  \n",
       "17595       [messed, saying, black, not, deserve, right]  \n",
       "\n",
       "[17596 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d39562da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1874)\t0.38655864797204015\n",
      "  (0, 2565)\t0.3369183517812025\n",
      "  (0, 1964)\t0.4085287030056687\n",
      "  (0, 4277)\t0.49396554919123437\n",
      "  (0, 1970)\t0.26594905432797417\n",
      "  (0, 4470)\t0.15796206623433257\n",
      "  (0, 4484)\t0.327463512592553\n",
      "  (0, 1103)\t0.35106624018460947\n",
      "  (1, 3988)\t0.46764828617011794\n",
      "  (1, 4455)\t0.10452774819368486\n",
      "  (1, 2633)\t0.18460642886856596\n",
      "  (1, 939)\t0.49051943994950004\n",
      "  (1, 2405)\t0.2687361424930761\n",
      "  (1, 248)\t0.2965908260733827\n",
      "  (1, 1750)\t0.1649905427170199\n",
      "  (1, 4221)\t0.3416575901596179\n",
      "  (1, 4856)\t0.2044196148320207\n",
      "  (1, 64)\t0.21152394159488963\n",
      "  (1, 694)\t0.29866739942996134\n",
      "  (1, 3066)\t0.12578228405018932\n",
      "  (2, 2674)\t0.5587285254775806\n",
      "  (2, 2175)\t0.5796117172772546\n",
      "  (2, 1946)\t0.548976877964676\n",
      "  (2, 3066)\t0.22471555236057383\n",
      "  (3, 209)\t0.8296004517693428\n",
      "  :\t:\n",
      "  (17594, 8)\t0.16430283962091688\n",
      "  (17594, 14)\t0.14253051698074895\n",
      "  (17594, 1555)\t0.12228340995633794\n",
      "  (17594, 4482)\t0.08772139759125756\n",
      "  (17594, 384)\t0.1023801384482584\n",
      "  (17594, 3120)\t0.14614376962658285\n",
      "  (17594, 3879)\t0.08512710809828576\n",
      "  (17594, 244)\t0.08803569880278857\n",
      "  (17594, 438)\t0.07570268639334474\n",
      "  (17594, 1304)\t0.08882565208686631\n",
      "  (17594, 2021)\t0.05841254368576046\n",
      "  (17594, 3166)\t0.07869477348785708\n",
      "  (17594, 1905)\t0.08835506506779571\n",
      "  (17594, 281)\t0.04371633404598651\n",
      "  (17594, 4978)\t0.10295090896867318\n",
      "  (17594, 4856)\t0.0718204774493649\n",
      "  (17594, 64)\t0.07431650083970749\n",
      "  (17594, 694)\t0.10493335115246465\n",
      "  (17594, 3066)\t0.13257647074493709\n",
      "  (17595, 2819)\t0.6316531031342569\n",
      "  (17595, 3880)\t0.4126509379813496\n",
      "  (17595, 1199)\t0.44771802006845846\n",
      "  (17595, 3790)\t0.34786010578750365\n",
      "  (17595, 523)\t0.2825701551414922\n",
      "  (17595, 3066)\t0.17155054478134218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "texts = data['Text'].values\n",
    "labels = data['Label'].values\n",
    "def tfidf_embedding(texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(texts)\n",
    "print(embeddings_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "64b1c4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.6792613636363637\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75      2094\n",
      "           1       0.63      0.50      0.56      1426\n",
      "\n",
      "    accuracy                           0.68      3520\n",
      "   macro avg       0.67      0.65      0.65      3520\n",
      "weighted avg       0.67      0.68      0.67      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(embeddings_tfidf, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_evaluate_rf(X_train_emb, X_test_emb, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_emb, y_train)\n",
    "    y_pred = rf.predict(X_test_emb)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report\n",
    "\n",
    "accuracy_tfidf, report_tfidf = train_evaluate_rf(X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf)\n",
    "print(f'TF-IDF Accuracy: {accuracy_tfidf}')\n",
    "print(f'TF-IDF Classification Report:\\n{report_tfidf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "05529e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34244603  0.56320775 -0.13862239 ... -0.64171565  0.26037344\n",
      "   0.11035831]\n",
      " [-0.54921669  0.89830345 -0.23524024 ... -0.73087049  0.3274487\n",
      "   0.01425049]\n",
      " [-0.55216449  0.81007117 -0.21898635 ... -0.89701265  0.2908527\n",
      "   0.02773679]\n",
      " ...\n",
      " [-0.30935851  0.54940945 -0.15352981 ... -0.64945304  0.41857535\n",
      "   0.0172969 ]\n",
      " [-0.46842033  0.73636758 -0.18529943 ... -0.61124378  0.25507087\n",
      "  -0.02185149]\n",
      " [-0.45092741  0.78913182 -0.14448845 ... -0.68255621  0.19861913\n",
      "  -0.11676348]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec_embedding(texts):\n",
    "    model = Word2Vec(texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    word_vectors = model.wv\n",
    "\n",
    "    def get_word2vec_embeddings(text, word_vectors):\n",
    "        embeddings = [word_vectors[word] for word in text if word in word_vectors]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(100)\n",
    "\n",
    "    embeddings = np.array([get_word2vec_embeddings(text, word_vectors) for text in texts])\n",
    "    return embeddings\n",
    "\n",
    "embeddings_w2v = word2vec_embedding(data['Tokens'])\n",
    "print(embeddings_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "10b6c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4315)\t1\n",
      "  (0, 17087)\t1\n",
      "  (0, 17033)\t1\n",
      "  (0, 7589)\t1\n",
      "  (0, 16345)\t1\n",
      "  (0, 7542)\t1\n",
      "  (0, 9660)\t1\n",
      "  (0, 7162)\t1\n",
      "  (1, 11708)\t1\n",
      "  (1, 2841)\t1\n",
      "  (1, 432)\t1\n",
      "  (1, 18571)\t1\n",
      "  (1, 16163)\t1\n",
      "  (1, 6681)\t1\n",
      "  (1, 1153)\t1\n",
      "  (1, 9044)\t1\n",
      "  (1, 3732)\t1\n",
      "  (1, 9903)\t1\n",
      "  (1, 16979)\t1\n",
      "  (1, 15319)\t1\n",
      "  (2, 11708)\t1\n",
      "  (2, 7492)\t1\n",
      "  (2, 8307)\t1\n",
      "  (2, 10035)\t1\n",
      "  (3, 11708)\t1\n",
      "  :\t:\n",
      "  (17594, 14863)\t1\n",
      "  (17594, 11962)\t1\n",
      "  (17594, 1655)\t1\n",
      "  (17594, 17080)\t1\n",
      "  (17594, 6028)\t1\n",
      "  (17594, 136)\t1\n",
      "  (17594, 67)\t1\n",
      "  (17594, 5843)\t1\n",
      "  (17594, 3671)\t1\n",
      "  (17594, 13871)\t1\n",
      "  (17594, 9153)\t1\n",
      "  (17594, 16205)\t1\n",
      "  (17594, 14867)\t1\n",
      "  (17594, 10620)\t1\n",
      "  (17594, 711)\t1\n",
      "  (17594, 84)\t1\n",
      "  (17594, 11347)\t1\n",
      "  (17594, 1122)\t1\n",
      "  (17594, 14409)\t1\n",
      "  (17595, 11708)\t1\n",
      "  (17595, 2122)\t1\n",
      "  (17595, 14463)\t1\n",
      "  (17595, 4683)\t1\n",
      "  (17595, 14867)\t1\n",
      "  (17595, 10648)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def one_hot_encoding(texts):\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_one_hot = one_hot_encoding(texts)\n",
    "X_train_one_hot, X_test_one_hot, y_train_one_hot, y_test_one_hot = train_test_split(embeddings_one_hot, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(embeddings_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b5cc49d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4315)\t1\n",
      "  (0, 17087)\t1\n",
      "  (0, 17033)\t1\n",
      "  (0, 7589)\t1\n",
      "  (0, 16345)\t1\n",
      "  (0, 7542)\t1\n",
      "  (0, 9660)\t1\n",
      "  (0, 7162)\t1\n",
      "  (1, 11708)\t1\n",
      "  (1, 2841)\t1\n",
      "  (1, 432)\t1\n",
      "  (1, 18571)\t1\n",
      "  (1, 16163)\t1\n",
      "  (1, 6681)\t1\n",
      "  (1, 1153)\t1\n",
      "  (1, 9044)\t1\n",
      "  (1, 3732)\t1\n",
      "  (1, 9903)\t1\n",
      "  (1, 16979)\t1\n",
      "  (1, 15319)\t1\n",
      "  (2, 11708)\t1\n",
      "  (2, 7492)\t1\n",
      "  (2, 8307)\t1\n",
      "  (2, 10035)\t1\n",
      "  (3, 11708)\t1\n",
      "  :\t:\n",
      "  (17594, 14863)\t1\n",
      "  (17594, 11962)\t1\n",
      "  (17594, 1655)\t1\n",
      "  (17594, 17080)\t1\n",
      "  (17594, 6028)\t1\n",
      "  (17594, 136)\t1\n",
      "  (17594, 67)\t1\n",
      "  (17594, 5843)\t1\n",
      "  (17594, 3671)\t1\n",
      "  (17594, 13871)\t1\n",
      "  (17594, 9153)\t1\n",
      "  (17594, 16205)\t1\n",
      "  (17594, 14867)\t1\n",
      "  (17594, 10620)\t1\n",
      "  (17594, 711)\t3\n",
      "  (17594, 84)\t1\n",
      "  (17594, 11347)\t1\n",
      "  (17594, 1122)\t4\n",
      "  (17594, 14409)\t1\n",
      "  (17595, 11708)\t1\n",
      "  (17595, 2122)\t1\n",
      "  (17595, 14463)\t1\n",
      "  (17595, 4683)\t1\n",
      "  (17595, 14867)\t1\n",
      "  (17595, 10648)\t1\n"
     ]
    }
   ],
   "source": [
    "def term_frequency_encoding(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_tf = term_frequency_encoding(texts)\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(embeddings_tf, labels, test_size=0.2, random_state=42)\n",
    "print(embeddings_tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
