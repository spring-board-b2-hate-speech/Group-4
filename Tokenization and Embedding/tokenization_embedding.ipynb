{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869b9ed4",
   "metadata": {},
   "source": [
    "Word Tokenization:\n",
    "word_tokenize is a function in Python that splits a given sentence into words using the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c08d094a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\balui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['Tokens']=data['Text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e63ab",
   "metadata": {},
   "source": [
    "Rule-Based Tokenization: This uses predefined rules to handle complex cases like contractions, hyphenated words, and special characters. (e.g., Penn Treebank Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3379c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "def rule_based_tokenize(text):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "data['token_rbt']=data['Text'].apply(rule_based_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb42709",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE): This subword tokenization method iteratively merges the most frequent character pairs to create a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ed7cc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def bpe_tokenize(text):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    return tokenizer.tokenize(text)\n",
    "data['token_bpe']=data['Text'].apply(rule_based_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f3a3ae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>token_rbt</th>\n",
       "      <th>token_bpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damn thought they had strict gun law germany</td>\n",
       "      <td>0</td>\n",
       "      <td>[damn, thought, they, had, strict, gun, law, g...</td>\n",
       "      <td>[damn, thought, they, had, strict, gun, law, g...</td>\n",
       "      <td>[damn, thought, they, had, strict, gun, law, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not care about what stand for anything its con...</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, care, about, what, stand, for, anything,...</td>\n",
       "      <td>[not, care, about, what, stand, for, anything,...</td>\n",
       "      <td>[not, care, about, what, stand, for, anything,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not group idea lol</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, group, idea, lol]</td>\n",
       "      <td>[not, group, idea, lol]</td>\n",
       "      <td>[not, group, idea, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not just america</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, just, america]</td>\n",
       "      <td>[not, just, america]</td>\n",
       "      <td>[not, just, america]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the dog spectacular dancer considering has two...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, dog, spectacular, dancer, considering, h...</td>\n",
       "      <td>[the, dog, spectacular, dancer, considering, h...</td>\n",
       "      <td>[the, dog, spectacular, dancer, considering, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>find rat nicer and cleaner than most chinese</td>\n",
       "      <td>1</td>\n",
       "      <td>[find, rat, nicer, and, cleaner, than, most, c...</td>\n",
       "      <td>[find, rat, nicer, and, cleaner, than, most, c...</td>\n",
       "      <td>[find, rat, nicer, and, cleaner, than, most, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>check out this niggar they hit thing like wild...</td>\n",
       "      <td>1</td>\n",
       "      <td>[check, out, this, niggar, they, hit, thing, l...</td>\n",
       "      <td>[check, out, this, niggar, they, hit, thing, l...</td>\n",
       "      <td>[check, out, this, niggar, they, hit, thing, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>this country has become absolute shamble the a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, country, has, become, absolute, shamble...</td>\n",
       "      <td>[this, country, has, become, absolute, shamble...</td>\n",
       "      <td>[this, country, has, become, absolute, shamble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>aged 16 antisemitism bad aged 18 antisemitism ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[aged, 16, antisemitism, bad, aged, 18, antise...</td>\n",
       "      <td>[aged, 16, antisemitism, bad, aged, 18, antise...</td>\n",
       "      <td>[aged, 16, antisemitism, bad, aged, 18, antise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>messed saying black not deserve right</td>\n",
       "      <td>0</td>\n",
       "      <td>[messed, saying, black, not, deserve, right]</td>\n",
       "      <td>[messed, saying, black, not, deserve, right]</td>\n",
       "      <td>[messed, saying, black, not, deserve, right]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17596 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label  \\\n",
       "0           damn thought they had strict gun law germany      0   \n",
       "1      not care about what stand for anything its con...      0   \n",
       "2                                     not group idea lol      0   \n",
       "3                                       not just america      0   \n",
       "4      the dog spectacular dancer considering has two...      0   \n",
       "...                                                  ...    ...   \n",
       "17591       find rat nicer and cleaner than most chinese      1   \n",
       "17592  check out this niggar they hit thing like wild...      1   \n",
       "17593  this country has become absolute shamble the a...      0   \n",
       "17594  aged 16 antisemitism bad aged 18 antisemitism ...      1   \n",
       "17595              messed saying black not deserve right      0   \n",
       "\n",
       "                                                  Tokens  \\\n",
       "0      [damn, thought, they, had, strict, gun, law, g...   \n",
       "1      [not, care, about, what, stand, for, anything,...   \n",
       "2                                [not, group, idea, lol]   \n",
       "3                                   [not, just, america]   \n",
       "4      [the, dog, spectacular, dancer, considering, h...   \n",
       "...                                                  ...   \n",
       "17591  [find, rat, nicer, and, cleaner, than, most, c...   \n",
       "17592  [check, out, this, niggar, they, hit, thing, l...   \n",
       "17593  [this, country, has, become, absolute, shamble...   \n",
       "17594  [aged, 16, antisemitism, bad, aged, 18, antise...   \n",
       "17595       [messed, saying, black, not, deserve, right]   \n",
       "\n",
       "                                               token_rbt  \\\n",
       "0      [damn, thought, they, had, strict, gun, law, g...   \n",
       "1      [not, care, about, what, stand, for, anything,...   \n",
       "2                                [not, group, idea, lol]   \n",
       "3                                   [not, just, america]   \n",
       "4      [the, dog, spectacular, dancer, considering, h...   \n",
       "...                                                  ...   \n",
       "17591  [find, rat, nicer, and, cleaner, than, most, c...   \n",
       "17592  [check, out, this, niggar, they, hit, thing, l...   \n",
       "17593  [this, country, has, become, absolute, shamble...   \n",
       "17594  [aged, 16, antisemitism, bad, aged, 18, antise...   \n",
       "17595       [messed, saying, black, not, deserve, right]   \n",
       "\n",
       "                                               token_bpe  \n",
       "0      [damn, thought, they, had, strict, gun, law, g...  \n",
       "1      [not, care, about, what, stand, for, anything,...  \n",
       "2                                [not, group, idea, lol]  \n",
       "3                                   [not, just, america]  \n",
       "4      [the, dog, spectacular, dancer, considering, h...  \n",
       "...                                                  ...  \n",
       "17591  [find, rat, nicer, and, cleaner, than, most, c...  \n",
       "17592  [check, out, this, niggar, they, hit, thing, l...  \n",
       "17593  [this, country, has, become, absolute, shamble...  \n",
       "17594  [aged, 16, antisemitism, bad, aged, 18, antise...  \n",
       "17595       [messed, saying, black, not, deserve, right]  \n",
       "\n",
       "[17596 rows x 5 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75060460",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Text'].values\n",
    "labels = data['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739d996",
   "metadata": {},
   "source": [
    "# Count based embeddings (Non-context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7399e",
   "metadata": {},
   "source": [
    "# One Hot Encoding: \n",
    "    Converts text data to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c2f6aec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4315)\t1\n",
      "  (0, 17087)\t1\n",
      "  (0, 17033)\t1\n",
      "  (0, 7589)\t1\n",
      "  (0, 16345)\t1\n",
      "  (0, 7542)\t1\n",
      "  (0, 9660)\t1\n",
      "  (0, 7162)\t1\n",
      "  (1, 11708)\t1\n",
      "  (1, 2841)\t1\n",
      "  (1, 432)\t1\n",
      "  (1, 18571)\t1\n",
      "  (1, 16163)\t1\n",
      "  (1, 6681)\t1\n",
      "  (1, 1153)\t1\n",
      "  (1, 9044)\t1\n",
      "  (1, 3732)\t1\n",
      "  (1, 9903)\t1\n",
      "  (1, 16979)\t1\n",
      "  (1, 15319)\t1\n",
      "  (2, 11708)\t1\n",
      "  (2, 7492)\t1\n",
      "  (2, 8307)\t1\n",
      "  (2, 10035)\t1\n",
      "  (3, 11708)\t1\n",
      "  :\t:\n",
      "  (17594, 14863)\t1\n",
      "  (17594, 11962)\t1\n",
      "  (17594, 1655)\t1\n",
      "  (17594, 17080)\t1\n",
      "  (17594, 6028)\t1\n",
      "  (17594, 136)\t1\n",
      "  (17594, 67)\t1\n",
      "  (17594, 5843)\t1\n",
      "  (17594, 3671)\t1\n",
      "  (17594, 13871)\t1\n",
      "  (17594, 9153)\t1\n",
      "  (17594, 16205)\t1\n",
      "  (17594, 14867)\t1\n",
      "  (17594, 10620)\t1\n",
      "  (17594, 711)\t1\n",
      "  (17594, 84)\t1\n",
      "  (17594, 11347)\t1\n",
      "  (17594, 1122)\t1\n",
      "  (17594, 14409)\t1\n",
      "  (17595, 11708)\t1\n",
      "  (17595, 2122)\t1\n",
      "  (17595, 14463)\t1\n",
      "  (17595, 4683)\t1\n",
      "  (17595, 14867)\t1\n",
      "  (17595, 10648)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def one_hot_encoding(texts):\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_one_hot = one_hot_encoding(texts)\n",
    "X_train_one_hot, X_test_one_hot, y_train_one_hot, y_test_one_hot = train_test_split(embeddings_one_hot, labels, test_size=0.2, random_state=42)\n",
    "print(embeddings_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a7e5f",
   "metadata": {},
   "source": [
    "# Term Frequency: \n",
    "    Converts text data to term frequency vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d1593760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4315)\t1\n",
      "  (0, 17087)\t1\n",
      "  (0, 17033)\t1\n",
      "  (0, 7589)\t1\n",
      "  (0, 16345)\t1\n",
      "  (0, 7542)\t1\n",
      "  (0, 9660)\t1\n",
      "  (0, 7162)\t1\n",
      "  (1, 11708)\t1\n",
      "  (1, 2841)\t1\n",
      "  (1, 432)\t1\n",
      "  (1, 18571)\t1\n",
      "  (1, 16163)\t1\n",
      "  (1, 6681)\t1\n",
      "  (1, 1153)\t1\n",
      "  (1, 9044)\t1\n",
      "  (1, 3732)\t1\n",
      "  (1, 9903)\t1\n",
      "  (1, 16979)\t1\n",
      "  (1, 15319)\t1\n",
      "  (2, 11708)\t1\n",
      "  (2, 7492)\t1\n",
      "  (2, 8307)\t1\n",
      "  (2, 10035)\t1\n",
      "  (3, 11708)\t1\n",
      "  :\t:\n",
      "  (17594, 14863)\t1\n",
      "  (17594, 11962)\t1\n",
      "  (17594, 1655)\t1\n",
      "  (17594, 17080)\t1\n",
      "  (17594, 6028)\t1\n",
      "  (17594, 136)\t1\n",
      "  (17594, 67)\t1\n",
      "  (17594, 5843)\t1\n",
      "  (17594, 3671)\t1\n",
      "  (17594, 13871)\t1\n",
      "  (17594, 9153)\t1\n",
      "  (17594, 16205)\t1\n",
      "  (17594, 14867)\t1\n",
      "  (17594, 10620)\t1\n",
      "  (17594, 711)\t3\n",
      "  (17594, 84)\t1\n",
      "  (17594, 11347)\t1\n",
      "  (17594, 1122)\t4\n",
      "  (17594, 14409)\t1\n",
      "  (17595, 11708)\t1\n",
      "  (17595, 2122)\t1\n",
      "  (17595, 14463)\t1\n",
      "  (17595, 4683)\t1\n",
      "  (17595, 14867)\t1\n",
      "  (17595, 10648)\t1\n"
     ]
    }
   ],
   "source": [
    "def term_frequency_encoding(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_tf = term_frequency_encoding(texts)\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(embeddings_tf, labels, test_size=0.2, random_state=42)\n",
    "print(embeddings_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da9503",
   "metadata": {},
   "source": [
    "# TF-IDF:\n",
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d39562da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7162)\t0.38655864797204015\n",
      "  (0, 9660)\t0.3369183517812025\n",
      "  (0, 7542)\t0.4085287030056687\n",
      "  (0, 16345)\t0.49396554919123437\n",
      "  (0, 7589)\t0.26594905432797417\n",
      "  (0, 17033)\t0.15796206623433257\n",
      "  (0, 17087)\t0.327463512592553\n",
      "  (0, 4315)\t0.35106624018460947\n",
      "  (1, 15319)\t0.46764828617011794\n",
      "  (1, 16979)\t0.10452774819368486\n",
      "  (1, 9903)\t0.18460642886856596\n",
      "  (1, 3732)\t0.49051943994950004\n",
      "  (1, 9044)\t0.2687361424930761\n",
      "  (1, 1153)\t0.2965908260733827\n",
      "  (1, 6681)\t0.1649905427170199\n",
      "  (1, 16163)\t0.3416575901596179\n",
      "  (1, 18571)\t0.2044196148320207\n",
      "  (1, 432)\t0.21152394159488963\n",
      "  (1, 2841)\t0.29866739942996134\n",
      "  (1, 11708)\t0.12578228405018932\n",
      "  (2, 10035)\t0.5587285254775806\n",
      "  (2, 8307)\t0.5796117172772546\n",
      "  (2, 7492)\t0.548976877964676\n",
      "  (2, 11708)\t0.22471555236057383\n",
      "  (3, 923)\t0.8296004517693428\n",
      "  :\t:\n",
      "  (17594, 67)\t0.16153022657761285\n",
      "  (17594, 136)\t0.14012531222980573\n",
      "  (17594, 6028)\t0.12021987545987466\n",
      "  (17594, 17080)\t0.08624109760557543\n",
      "  (17594, 1655)\t0.10065247197644427\n",
      "  (17594, 11962)\t0.14367759117952028\n",
      "  (17594, 14863)\t0.08369058678924071\n",
      "  (17594, 1143)\t0.08655009497914094\n",
      "  (17594, 1869)\t0.0744252023511236\n",
      "  (17594, 5126)\t0.08732671778893056\n",
      "  (17594, 7769)\t0.05742683107793596\n",
      "  (17594, 12145)\t0.07736679792811813\n",
      "  (17594, 7294)\t0.0868640719333243\n",
      "  (17594, 1285)\t0.04297862021060185\n",
      "  (17594, 19019)\t0.10121361073520913\n",
      "  (17594, 18571)\t0.07060850574509082\n",
      "  (17594, 432)\t0.07306240870084797\n",
      "  (17594, 2841)\t0.10316259917548003\n",
      "  (17594, 11708)\t0.13033924068323685\n",
      "  (17595, 10648)\t0.6316531031342569\n",
      "  (17595, 14867)\t0.4126509379813496\n",
      "  (17595, 4683)\t0.44771802006845846\n",
      "  (17595, 14463)\t0.34786010578750365\n",
      "  (17595, 2122)\t0.2825701551414922\n",
      "  (17595, 11708)\t0.17155054478134218\n",
      "(17596, 19209)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_embedding(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(texts)\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(embeddings_tfidf, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(embeddings_tfidf)\n",
    "print(embeddings_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b887f7",
   "metadata": {},
   "source": [
    "# Position based embeddings (context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0c173",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Word2Vec transforms each word into a dense vector of fixed size. It captures semantic meanings by training on large text corpora. Word2Vec creates vectors of the words that are distributed numerical representations of word features â€“ these word features could comprise of words that represent the context of the individual words present in our vocabulary.\n",
    "\n",
    "Two different model architectures that can be used by Word2Vec to create the word embeddings are the Continuous Bag of Words (CBOW) model & the Skip-Gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52f94b",
   "metadata": {},
   "source": [
    "# CBOW\n",
    "In CBOW the words occurring in context (surrounding words) of a selected word are used as inputs and middle or selected word as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "05529e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=100, 19219 keys>\n",
      "[[-0.38090506  0.5571242  -0.15338194 ... -0.62807012  0.29678026\n",
      "   0.08190011]\n",
      " [-0.55611628  0.8685289  -0.21476047 ... -0.72417134  0.33842731\n",
      "  -0.02355094]\n",
      " [-0.57028615  0.80628884 -0.24882451 ... -0.87437671  0.30924293\n",
      "   0.01526066]\n",
      " ...\n",
      " [-0.32567596  0.48062518 -0.14309759 ... -0.6181466   0.41403186\n",
      "   0.00197476]\n",
      " [-0.4748036   0.72154164 -0.17764334 ... -0.61682618  0.28610817\n",
      "  -0.05201351]\n",
      " [-0.46838364  0.80140084 -0.15601604 ... -0.69164872  0.26285425\n",
      "  -0.12802459]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec_embedding_cbow(texts):\n",
    "    model = Word2Vec(texts, vector_size=100, window=5, min_count=1, workers=4,sg=0)\n",
    "    word_vectors = model.wv\n",
    "    #print(word_vectors)\n",
    "\n",
    "    def get_word2vec_embeddings(text, word_vectors):\n",
    "        embeddings = [word_vectors[word] for word in text if word in word_vectors]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(100)\n",
    "\n",
    "    embeddings = np.array([get_word2vec_embeddings(text, word_vectors) for text in texts])\n",
    "    return embeddings\n",
    "\n",
    "embeddings_w2v_cbow = word2vec_embedding(data['Tokens'])\n",
    "print(embeddings_w2v_cbow)\n",
    "X_train_w2v1, X_test_w2v1, y_train_w2v1, y_test_w2v1 = train_test_split(embeddings_w2v_cbow, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d90e5b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=100, 19219 keys>\n",
      "[[-0.39908716  0.61797488 -0.16120888 ... -0.66573697  0.2926228\n",
      "   0.12767859]\n",
      " [-0.51712316  0.86545926 -0.27176052 ... -0.71439403  0.32241449\n",
      "  -0.01344244]\n",
      " [-0.53488934  0.84057909 -0.26861066 ... -0.92627698  0.25765699\n",
      "   0.0280109 ]\n",
      " ...\n",
      " [-0.32288796  0.5558821  -0.14520247 ... -0.63530171  0.45343384\n",
      "   0.02544812]\n",
      " [-0.4305416   0.71749461 -0.21778882 ... -0.61553448  0.22991703\n",
      "  -0.04582477]\n",
      " [-0.4069232   0.77720189 -0.21569127 ... -0.68651742  0.14261054\n",
      "  -0.10453237]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec_embedding_sg(texts):\n",
    "    model = Word2Vec(texts, vector_size=100, window=5, min_count=1, workers=4,sg=1)\n",
    "    word_vectors = model.wv\n",
    "    #print(word_vectors)\n",
    "\n",
    "    def get_word2vec_embeddings(text, word_vectors):\n",
    "        embeddings = [word_vectors[word] for word in text if word in word_vectors]\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(100)\n",
    "\n",
    "    embeddings = np.array([get_word2vec_embeddings(text, word_vectors) for text in texts])\n",
    "    return embeddings\n",
    "\n",
    "embeddings_w2v_sg = word2vec_embedding(data['Tokens'])\n",
    "print(embeddings_w2v_sg)\n",
    "X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2 = train_test_split(embeddings_w2v_sg, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "64b1c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_evaluate_rf(X_train_emb, X_test_emb, y_train, y_test):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train_emb, y_train)\n",
    "    y_pred = rf.predict(X_test_emb)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dc5ea808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding Accuracy: 0.6795454545454546\n",
      "One Hot Encoding Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75      2094\n",
      "           1       0.63      0.50      0.56      1426\n",
      "\n",
      "    accuracy                           0.68      3520\n",
      "   macro avg       0.67      0.65      0.65      3520\n",
      "weighted avg       0.67      0.68      0.67      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_one_hot, report_one_hot = train_evaluate_rf(X_train_one_hot, X_test_one_hot, y_train_one_hot, y_test_one_hot)\n",
    "print(f'One Hot Encoding Accuracy: {accuracy_one_hot}')\n",
    "print(f'One Hot Encoding Classification Report:\\n{report_one_hot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d7794007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Accuracy: 0.6713068181818181\n",
      "Term Frequency Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74      2094\n",
      "           1       0.62      0.48      0.54      1426\n",
      "\n",
      "    accuracy                           0.67      3520\n",
      "   macro avg       0.66      0.64      0.64      3520\n",
      "weighted avg       0.66      0.67      0.66      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_tf, report_tf = train_evaluate_rf(X_train_tf, X_test_tf, y_train_tf, y_test_tf)\n",
    "print(f'Term Frequency Accuracy: {accuracy_tf}')\n",
    "print(f'Term Frequency Classification Report:\\n{report_tf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5d8a2958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.684375\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.83      0.76      2094\n",
      "           1       0.65      0.47      0.55      1426\n",
      "\n",
      "    accuracy                           0.68      3520\n",
      "   macro avg       0.68      0.65      0.65      3520\n",
      "weighted avg       0.68      0.68      0.67      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_tfidf, report_tfidf = train_evaluate_rf(X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf)\n",
    "print(f'TF-IDF Accuracy: {accuracy_tfidf}')\n",
    "print(f'TF-IDF Classification Report:\\n{report_tfidf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d9ad3eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Accuracy: 0.6363636363636364\n",
      "Word2Vec Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.82      0.73      2094\n",
      "           1       0.58      0.36      0.45      1426\n",
      "\n",
      "    accuracy                           0.64      3520\n",
      "   macro avg       0.62      0.59      0.59      3520\n",
      "weighted avg       0.63      0.64      0.61      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_w2v1, report_w2v1 = train_evaluate_rf(X_train_w2v1, X_test_w2v1, y_train_w2v1, y_test_w2v1)\n",
    "print(f'Word2Vec Accuracy: {accuracy_w2v1}')\n",
    "print(f'Word2Vec Classification Report:\\n{report_w2v1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9bb259f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Accuracy: 0.6284090909090909\n",
      "Word2Vec Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.82      0.72      2094\n",
      "           1       0.57      0.35      0.43      1426\n",
      "\n",
      "    accuracy                           0.63      3520\n",
      "   macro avg       0.61      0.58      0.58      3520\n",
      "weighted avg       0.62      0.63      0.61      3520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_w2v2, report_w2v2 = train_evaluate_rf(X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2)\n",
    "print(f'Word2Vec Accuracy: {accuracy_w2v2}')\n",
    "print(f'Word2Vec Classification Report:\\n{report_w2v2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e301083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
